{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FKJS791dov9f"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tqdm \n",
    "\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69563,
     "status": "ok",
     "timestamp": 1623909239857,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "lVENHMCJ8KDt",
    "outputId": "432d2059-d283-46ac-bbfe-b9b9b5eec905"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vermashresth/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/Users/vermashresth/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BLACK BOX: This cell is a black box from Lovish that reads/parses the\\\n",
    "beneficiary data and returns arrays corresponding to beneficiary IDs,\\ \n",
    "cluster assignments, transition probs, whittle indices\n",
    "\"\"\"\n",
    "\n",
    "def loadBeneficiaryData():\n",
    "  \"\"\"\n",
    "  This function cd's to the folder where the relevant data pickle files are \\\n",
    "  stored. Reads and returns:\n",
    "\n",
    "  rmab_grou_probs:\n",
    "    Numpy matrix storing transition probabilites of size Nx2x2x2 with axes \\\n",
    "    num_beneficiariesX action X start_state x end_state\n",
    "    N: num of beneficiaries\n",
    "    action: 0= passive, 1=active\n",
    "    start/end state: 0= Not engaging 1=Engaging\n",
    "\n",
    "  rmab_group_whittle_indices:\n",
    "    Numpy array of size Nx2. i-th entry corresponds to the whittle indices\\\n",
    "    for states [NE, E] for beneficiary i\n",
    "  \n",
    "  engagement_matrix: \n",
    "    Dictionary with keys: ['rmab', 'round_robin', 'control']. \n",
    "    engagement_matrix[key] is a NxL numpy matrix storing binary engagement data\n",
    "    N=num of beneficiaries\n",
    "    L=num of timesteps  \n",
    "  \"\"\"\n",
    "#   from google.colab import drive\n",
    "#   drive.mount('/content/drive')\n",
    "  '''\n",
    "  This line reads files present in my drive. For it to work on your system,\n",
    "  please make sure to have the necessary files present in your cwd.\n",
    "  '''\n",
    "  ## Loading Whittle Policy related files\n",
    "  aug_states = []\n",
    "  for i in range(6):\n",
    "      if i % 2 == 0:\n",
    "          aug_states.append('E{}'.format(i // 2))\n",
    "      else:\n",
    "          aug_states.append('NE{}'.format(i // 2))\n",
    "\n",
    "  CONFIG = {\n",
    "      \"problem\": {\n",
    "          \"orig_states\": ['E', 'NE'],\n",
    "          \"states\": aug_states + ['E', 'NE'],\n",
    "          \"actions\": [\"A\", \"I\"],\n",
    "      },\n",
    "      \"time_step\": 7,\n",
    "      \"gamma\": 0.99\n",
    "  }\n",
    "\n",
    "#   %cd drive/My\\ Drive/ARMMAN/code/ \n",
    "  with open('policy_dump.pkl', 'rb') as fr:\n",
    "    pilot_user_ids, pilot_static_features, cls, cluster_transition_probabilities, m_values, q_values = pickle.load(fr)\n",
    "  fr.close()\n",
    "\n",
    "#   #engagement_matrix_file='full_matrix.pkl'\n",
    "#   engagement_matrix_file='full_matrix_week9_end.pkl'\n",
    "#   with open(engagement_matrix_file, 'rb') as fr:\n",
    "#     engagement_matrix = pickle.load(fr)\n",
    "#   fr.close()\n",
    "\n",
    "  cluster_assignments = cls.predict(pilot_static_features)\n",
    "\n",
    "  rmab_group_results = pd.read_csv('outputs/pilot_outputs/rmab_pilot.csv')\n",
    "  rmab_user_ids = rmab_group_results['user_id'].to_list()\n",
    "\n",
    "  rmab_group_probs, rmab_group_whittle_indices = [], []\n",
    "\n",
    "  for idx, user_id in enumerate(rmab_user_ids):\n",
    "    locate_idx = np.where(pilot_user_ids == user_id)[0][0]\n",
    "    curr_cluster = cluster_assignments[locate_idx]\n",
    "    whittle_indices = m_values[curr_cluster]\n",
    "\n",
    "    start_state = rmab_group_results[rmab_group_results['user_id'] == user_id]['start_state'].item()\n",
    "\n",
    "    t_probs = np.zeros((len(CONFIG['problem']['states']), len(CONFIG['problem']['states']), len(CONFIG['problem']['actions'])))\n",
    "    two_state_probs = np.zeros((2, 2, 2))\n",
    "    for i in range(two_state_probs.shape[0]):\n",
    "      for j in range(two_state_probs.shape[1]):\n",
    "        for k in range(two_state_probs.shape[2]):\n",
    "          s = CONFIG['problem']['orig_states'][i]\n",
    "          s_prime = CONFIG['problem']['orig_states'][j]\n",
    "          a = CONFIG['problem']['actions'][k]\n",
    "          #two_state_probs[i, j, k] = cluster_transition_probabilities.loc[cluster_transition_probabilities['cluster']==curr_cluster, \"P(\" + s + \", \" + a + \", \" + s_prime + \")\"]\n",
    "          two_state_probs[k, int(1-i), int(1-j)] = cluster_transition_probabilities.loc[cluster_transition_probabilities['cluster']==curr_cluster, \"P(\" + s + \", \" + a + \", \" + s_prime + \")\"]\n",
    "          # The indices are adjusted to return transition matrix in the format T[action, start_state, end_state] where state=0 is bad state and state=1 is good state\n",
    "          \n",
    "    t_probs[0 : 2, 2 : 4, 0] = two_state_probs[0, :, :]\n",
    "    t_probs[2 : 4, 4 : 6, 0] = two_state_probs[0, :, :]\n",
    "    t_probs[4 : 6, 6 : 8, 0] = two_state_probs[0, :, :]\n",
    "    t_probs[6 : 8, 6 : 8, 0] = two_state_probs[0, :, :]\n",
    "\n",
    "    t_probs[0 : 2, 2 : 4, 1] = two_state_probs[0, :, :]\n",
    "    t_probs[2 : 4, 4 : 6, 1] = two_state_probs[0, :, :]\n",
    "    t_probs[4 : 6, 6 : 8, 1] = two_state_probs[0, :, :]\n",
    "    t_probs[6 : 8, 0 : 2, 1] = two_state_probs[1, :, :]\n",
    "\n",
    "    rmab_group_probs.append(two_state_probs)\n",
    "    user_whittle_idx = rmab_group_results[rmab_group_results['user_id'] == user_id]['whittle_index'].item()\n",
    "    #rmab_group_whittle_indices.append(user_whittle_idx)\n",
    "    rmab_group_whittle_indices.append([whittle_indices[-1],whittle_indices[-2]]) ### whittle_indeces[-1] is for NE and whittle_indices[-2] is for E\n",
    "\n",
    "  rmab_group_probs = np.array(rmab_group_probs)\n",
    "  rmab_group_whittle_indices = np.array(rmab_group_whittle_indices)\n",
    "  return rmab_group_probs, rmab_group_whittle_indices\n",
    "T_data,w= loadBeneficiaryData()\n",
    "\n",
    "\n",
    "def get_reward(state, action, m):\n",
    "    \"\"\"\n",
    "    Helper function for plan2() function below\n",
    "    \"\"\"\n",
    "    if state[0] == \"L\":\n",
    "        reward = 1.0\n",
    "    else:\n",
    "        reward = -1.0\n",
    "    if action == 'N':\n",
    "        reward += m\n",
    "\n",
    "    return reward\n",
    "\n",
    "def convertAxis(T):\n",
    "    '''\n",
    "    convert T matrix from format: a, s, s' (where s=0 is bad state) --> \n",
    "                                  s, s', a (where s=0 is good state) \n",
    "    This function is needed because most of my simulator code uses the former\n",
    "    format while the whittleIndex function below is coded up using the latter. \n",
    "    '''\n",
    "    P=np.zeros_like(T)\n",
    "    for a in range(2):\n",
    "      for s in range(2):\n",
    "        for ss in range(2):\n",
    "          P[1-s,1-ss,a]=T[a,s,ss]\n",
    "    return P\n",
    "\n",
    "def getWhittleIndex(two_state_probs, sleeping_constraint = True ):\n",
    "    '''\n",
    "    Function that generates whittle indices given transition probabilities. \n",
    "    Inputs: \n",
    "      two_state_probs: Transition probability matrix with axes,\n",
    "                      action, starting_state, final_state. \n",
    "                      Here State=0 means engaging state\n",
    "      sleeping_constraint: If True, considers the frequency constraints \n",
    "                          (and the modified MDP)\n",
    "    Outputs:\n",
    "      A list of len 2, corresponding to whittle indices of [NE, E] states.\n",
    "    '''\n",
    "    two_state_probs=convertAxis(two_state_probs)\n",
    "\n",
    "    aug_states = []\n",
    "    for i in range(6):\n",
    "        if i % 2 == 0:\n",
    "            aug_states.append('L{}'.format(i // 2))\n",
    "        else:\n",
    "            aug_states.append('H{}'.format(i // 2))\n",
    "\n",
    "    if sleeping_constraint:\n",
    "        local_CONFIG = {\n",
    "            'problem': {\n",
    "                \"orig_states\": ['L', 'H'],\n",
    "                \"states\": aug_states + ['L', 'H'],\n",
    "                \"actions\": [\"N\", \"I\"],\n",
    "            },\n",
    "            \"time_step\": 7,\n",
    "            \"gamma\": 0.99,\n",
    "        }\n",
    "    else:\n",
    "        local_CONFIG = {\n",
    "            'problem': {\n",
    "                \"orig_states\": ['L', 'H'],\n",
    "                \"states\": ['L', 'H'],\n",
    "                \"actions\": [\"N\", \"I\"],\n",
    "            },\n",
    "            \"time_step\": 7,\n",
    "            \"gamma\": 0.99,\n",
    "        }\n",
    "\n",
    "    v_values = np.zeros(len(local_CONFIG['problem']['states']))\n",
    "    q_values = np.zeros((len(local_CONFIG['problem']['states']), \\\n",
    "                         len(local_CONFIG['problem']['actions'])))\n",
    "    high_m_values = 1 * np.ones(len(local_CONFIG['problem']['states']))\n",
    "    low_m_values = -1 * np.ones(len(local_CONFIG['problem']['states']))\n",
    "\n",
    "    t_probs = np.zeros((len(local_CONFIG['problem']['states']), \\\n",
    "                        len(local_CONFIG['problem']['states']), \\\n",
    "                        len(local_CONFIG['problem']['actions'])))\n",
    "\n",
    "    if sleeping_constraint:    \n",
    "        t_probs[0 : 2, 2 : 4, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[2 : 4, 4 : 6, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[4 : 6, 6 : 8, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[6 : 8, 6 : 8, 0] = two_state_probs[:, :, 0]\n",
    "\n",
    "        t_probs[0 : 2, 2 : 4, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[2 : 4, 4 : 6, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[4 : 6, 6 : 8, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[6 : 8, 0 : 2, 1] = two_state_probs[:, :, 1]\n",
    "    else:\n",
    "        t_probs = two_state_probs\n",
    "\n",
    "    max_q_diff = np.inf\n",
    "    prev_m_values, m_values = None, None\n",
    "    while max_q_diff > 1e-5:\n",
    "        prev_m_values = m_values\n",
    "        m_values = (low_m_values + high_m_values) / 2\n",
    "        if type(prev_m_values) != type(None) and \\\n",
    "        abs(prev_m_values - m_values).max() < 1e-20:\n",
    "            break\n",
    "        max_q_diff = 0\n",
    "        v_values = np.zeros((len(local_CONFIG['problem']['states'])))\n",
    "        q_values = np.zeros((len(local_CONFIG['problem']['states']), \\\n",
    "                             len(local_CONFIG['problem']['actions'])))\n",
    "        delta = np.inf\n",
    "        while delta > 0.0001:\n",
    "            delta = 0\n",
    "            for i in range(t_probs.shape[0]):\n",
    "                v = v_values[i]\n",
    "                v_a = np.zeros((t_probs.shape[2],))\n",
    "                for k in range(v_a.shape[0]):\n",
    "                    for j in range(t_probs.shape[1]):\n",
    "                        v_a[k] += t_probs[i, j, k] * \\\n",
    "                        (get_reward(local_CONFIG['problem']['states'][i], \\\n",
    "                                    local_CONFIG['problem']['actions'][k], \\\n",
    "                                    m_values[i]) + local_CONFIG[\"gamma\"] * \\\n",
    "                         v_values[j])\n",
    "\n",
    "                v_values[i] = np.max(v_a)\n",
    "                delta = max([delta, abs(v_values[i] - v)])\n",
    "\n",
    "        state_idx = -1\n",
    "        for state in range(q_values.shape[0]):\n",
    "            for action in range(q_values.shape[1]):\n",
    "                for next_state in range(q_values.shape[0]):\n",
    "                    q_values[state, action] += \\\n",
    "                    t_probs[state, next_state, action] * \\\n",
    "                    (get_reward(local_CONFIG['problem']['states'][state], \\\n",
    "                                local_CONFIG['problem']['actions'][action], \\\n",
    "                                m_values[state]) + local_CONFIG[\"gamma\"] * \\\n",
    "                     v_values[next_state])\n",
    "            # print(state, q_values[cluster, state, 0], \\\n",
    "            #q_values[cluster, state, 1])\n",
    "\n",
    "        for state in range(q_values.shape[0]):\n",
    "            if abs(q_values[state, 1] - q_values[state, 0]) > max_q_diff:\n",
    "                state_idx = state\n",
    "                max_q_diff = abs(q_values[state, 1] - q_values[state, 0])\n",
    "\n",
    "        # print(q_values)\n",
    "        # print(low_m_values, high_m_values)\n",
    "        if max_q_diff > 1e-5 and q_values[state_idx, 0] < q_values[state_idx, 1]:\n",
    "            low_m_values[state_idx] = m_values[state_idx]\n",
    "        elif max_q_diff > 1e-5 and q_values[state_idx, 0] > q_values[state_idx, 1]:\n",
    "            high_m_values[state_idx] = m_values[state_idx]\n",
    "\n",
    "        # print(low_m_values, high_m_values, state_idx)\n",
    "        # ipdb.set_trace()\n",
    "    \n",
    "    m_values = (low_m_values + high_m_values) / 2\n",
    "\n",
    "    #return q_values, m_values\n",
    "    return [m_values[-1], m_values[-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KetTT4HaLH-X"
   },
   "outputs": [],
   "source": [
    "\"\"\"This cell contains all utils-like helper functions\"\"\"\n",
    "\n",
    "def getTopk(a, k):\n",
    "  '''\n",
    "  Returns indices of top k elements in array a\n",
    "  '''\n",
    "  a=np.array(a)\n",
    "  return a.argsort()[-k:][::-1]\n",
    "\n",
    "def verify_T_matrix(T):\n",
    "    '''\n",
    "    Checks whether it satisfies the natural constraints assumed in the problem \\\n",
    "    (active is better than passive etc.)\n",
    "    '''\n",
    "    valid = True\n",
    "    valid &= T[0, 0, 1] <= T[0, 1, 1] #non-oscillate condition\n",
    "    valid &= T[1, 0, 1] <= T[1, 1, 1] #must be true for active as well\n",
    "    valid &= T[0, 1, 1] <= T[1, 1, 1] #action has positive \"maintenance\" value\n",
    "    valid &= T[0, 0, 1] <= T[1, 0, 1] #action has non-negative \"influence\" value\n",
    "    return valid\n",
    "\n",
    "def generateRandomTmatrix(N):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a Nx2x2x2 T matrix indexed as: \\\n",
    "    T[beneficiary_number][action][current_state][next_state]\n",
    "    action=0 denotes passive action, a=1 is active action\n",
    "    \"\"\"\n",
    "        \n",
    "    T=np.zeros((N,2,2,2))\n",
    "    for i in range(N):\n",
    "        p_pass_01, p_pass_11, p_act_01, p_act_11=np.random.uniform(size=4)\n",
    "        T[i,0]=np.array([[1-p_pass_01, p_pass_01],[1-p_pass_11, p_pass_11]])\n",
    "        T[i,1]=np.array([[1-p_act_01, p_act_01],[1-p_act_11, p_act_11]])\n",
    "    return T  \n",
    "\n",
    "def barPlot(labels, values, errors, ylabel='', title=None, figsize=(10,3.5)):\n",
    "    \"\"\"\n",
    "    Standard matplotlib bar plot code\n",
    "    \"\"\"\n",
    "    def autolabel(rects):\n",
    "        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\n",
    "        \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height),\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    colors=['#002222', '#335577', '#5599cc', '#bbddff', '#ddeeff', '#882222',\\\n",
    "            '#bb2222','#dd2222', '#ff2222' ]\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.65  # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for i in range(len(values)):\n",
    "      rects1=ax.bar(x[i], np.round(values[i], decimals=2) , width, \\\n",
    "                    yerr=errors[i], label=labels[i], align='center', \\\n",
    "                    color=colors[i], edgecolor='k', capsize=4)\n",
    "      autolabel(rects1) \n",
    "    ax.legend(loc='lower center', bbox_to_anchor=(0.5, 1.01),ncol=2)  \n",
    "    ax.set_ylabel(ylabel, fontsize=20)\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=16)   \n",
    "    ax.set_xticks(x)\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1, fontsize=14)\n",
    "    ax.set_ylim(top=max(values)*1.1)\n",
    "    autolabel(rects1)       \n",
    "    plt.tight_layout() \n",
    "    #plt.savefig(fname)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotIB(simulated_rewards, policies, policy_names,\n",
    "           lower_pol=0, upper_pol=3, policies_to_plot=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes intervention benefit from raw rewards, computes errorbars and \n",
    "    generates bar plots \n",
    "    Inputs: \n",
    "      simulated_rewards: A matrix of dimensions num_trials X num_policies\n",
    "      policies: Policies which are to be plotted. \n",
    "      policy_names: Dictionary which stores policy names as value with policy\n",
    "                   number as key.\n",
    "      lower_pol: Index 0 is policy 0 (lower baseline). \n",
    "      upper_pol: Policy 3, RR (Round Robin), is the default upper baseline\n",
    "      policies_to_plot: Policies to include in the plot. \n",
    "                  If None, then defaults to policies.\n",
    "    \"\"\"\n",
    "    lower_reward=np.mean(simulated_rewards[:,policies.index(lower_pol)])\n",
    "    upper_reward=np.mean(simulated_rewards[:,policies.index(upper_pol)])\n",
    "    lower_error=(np.std(simulated_rewards[:,policies.index(lower_pol)]))\\\n",
    "                  /(np.sqrt(len(simulated_rewards)))\n",
    "    upper_error=(np.std(simulated_rewards[:,policies.index(upper_pol)]))\\\n",
    "                  /(np.sqrt(len(simulated_rewards)))\n",
    "    if policies_to_plot is None: \n",
    "      policies_to_plot=policies\n",
    "    \n",
    "    ibs={}\n",
    "    errors_rewards={}\n",
    "    errors_ibs={}\n",
    "\n",
    "    for pol in policies_to_plot:\n",
    "      reward_gain=np.mean(simulated_rewards[:,policies.index(pol)])-lower_reward\n",
    "      ibs[pol]=(100.*(reward_gain))/(upper_reward-lower_reward)\n",
    "      errors_rewards[pol]=(np.std(simulated_rewards[:,policies.index(pol)]))\\\n",
    "                            /(np.sqrt(len(simulated_rewards)))\n",
    "      errors_ibs[pol]=((errors_rewards[pol]+lower_error)/(reward_gain) \\\n",
    "              +(upper_error+lower_error)/(upper_reward-lower_reward))*ibs[pol]\n",
    "\n",
    "    barPlot([policy_names[pol] for pol in policies_to_plot], \\\n",
    "            [ibs[pol] for pol in policies_to_plot], \\\n",
    "            [errors_ibs[pol] for pol in policies_to_plot], \\\n",
    "            ylabel='Intervention benefit')\n",
    "\n",
    "    return ibs, errors_ibs\n",
    "\n",
    "    \n",
    "def getCounterExampleTmatrix(N, badf):\n",
    "  \"\"\"\n",
    "  Generates transition matrices that lead to poor performance of myopic policy\n",
    "  Inputs: \n",
    "    N: Num of beneficiaries\n",
    "    badf: Fraction of bad (non-recoverable) beneficiaries. Other (1-badf) are \\\n",
    "    self-correcting beneficiaries.\n",
    "  Output: \n",
    "    T: Transition matrix of size Nx2x2x2 with axes denoting [num_benficiary]x\\\n",
    "      [action][start_state][end_state]\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  shift=0.05                        ### Magnitude of random perturbation   \n",
    "  T=np.zeros((N,2,2,2))\n",
    "  type1= [[[0.97, 0.03],\n",
    "            [0.03, 0.97]],\n",
    "\n",
    "          [[0.96, 0.04],\n",
    "            [0.01, 0.99]]]          ### Bad (non-recoverable) beneficiary\n",
    "                                    ### Whittle is supposed to select this\n",
    "  \n",
    "  type2 = [[[0.25, 0.75],\n",
    "              [0.03, 0.97]],\n",
    "          [[0.23, 0.77],\n",
    "            [0.01     , 0.99 ]]]    ### Good (self-healing) beneficiary\n",
    "                                    ### Mypoic is supposed to fall for this\n",
    "  \n",
    "  types=[type1, type2]\n",
    "  ### Select which beneficiary should be which type\n",
    "  type_choices=np.random.choice([0, 1],p=[badf/100., 1-(badf/100.)], size=N, \\\n",
    "                                replace=True)\n",
    "  \n",
    "  ### For each beneficiary, add random perturbation to make all T's unique\n",
    "  for i in range(N):\n",
    "    \n",
    "      T[i]=np.copy(np.array(types[int(type_choices[i])]))\n",
    "      # add benefit_act_00 to benefit_act_11 to guarantee the p11>p01 condition\n",
    "      \n",
    "      benefit_act_00=np.random.uniform(low=0., high=shift) \n",
    "      # will subtract from prob of staying 0,0\n",
    "      benefit_act_11= benefit_act_00 + np.random.uniform(low=0., high=shift) \n",
    "      # will add to prob of staying 1,1\n",
    "      penalty_pass_11=np.random.uniform(low=0., high=shift)\n",
    "      # will sub from prob of staying 1,1\n",
    "      penalty_pass_00=penalty_pass_11+np.random.uniform(low=0., high=shift) \n",
    "      # will add to prob of staying 0,0\n",
    "\n",
    "\n",
    "      T[i][1][0][0]= max(0, T[i][1][0][0] - benefit_act_00)            \n",
    "      T[i][1][1][1]= min(1, T[i][1][1][1] + benefit_act_11)            \n",
    "      \n",
    "      T[i][0][0][0]= min(1, T[i][0][0][0] + penalty_pass_00)            \n",
    "      T[i][0][1][1]= max(0, T[i][0][1][1] - penalty_pass_11)  \n",
    "      \n",
    "      T[i][0][0][1]=   1- T[i][0][0][0]          \n",
    "      T[i][0][1][0]=   1- T[i][0][1][1]\n",
    "      \n",
    "      T[i][1][0][1]=   1- T[i][1][0][0]          \n",
    "      T[i][1][1][0]=   1- T[i][1][1][1]\n",
    "  \n",
    "  return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "t2ci1Dc0rSvj"
   },
   "outputs": [],
   "source": [
    "def takeActions(states, T, actions):\n",
    "  '''\n",
    "  Simulates random transitions by flipping coins for each beneficiaries and \\\n",
    "  updating their states given previous states and actions taken\n",
    "\n",
    "  Inputs: \n",
    "  states: A vector of size N with binary values {0,1} respresenting the states\\\n",
    "          of beneficiaries\n",
    "  T: Transition matrix of size Nx2x2x2 (num_beneficiaries x action x \\\n",
    "          starting_state x ending_state)\n",
    "  actions: A vector of size N with binary values {0,1} representing action \\\n",
    "          chosen for each beneficiary\n",
    "\n",
    "  Outputs:\n",
    "  next_states:  A vector of size N with binary values {0,1} respresenting the\\\n",
    "          *next* states of beneficiaries, determined using coin tosses\n",
    "  '''\n",
    "\n",
    "  next_states=np.random.binomial(1, T[np.arange(T.shape[0]),actions.astype(int)\\\n",
    "                                      , states,1])\n",
    "  return next_states\n",
    "\n",
    "\n",
    "def getActions(states, T,k, policy=0, timestep=None,w=None, available_arms=None\\\n",
    "               , sleeping_constraint=True):\n",
    "  '''\n",
    "  Computes actions to be taken according to the given policy. \n",
    "  See below for more info on policy map\n",
    "\n",
    "  Inputs: \n",
    "    states: A vector of size N with binary values {0,1} respresenting the \\\n",
    "        states of beneficiaries\n",
    "    T: Transition matrix of size Nx2x2x2 \\\n",
    "        (num_beneficiaries x action x starting_state x ending_state)\n",
    "    k: Intervention budget. Assumed to be between 0 and num_of_beneficiaries\n",
    "    policy: Policy to implement. Dictionary: \\\n",
    "        {0: No interventions, 1: intervene on all, 2: random, 3: round-robin,\\\n",
    "        4: myopic, 5: whittle}\n",
    "    timestep: timestep at which function is called (relevant for round robin)\n",
    "    w: Array of size Nx2 storing the whittle indices of arms when they are \\\n",
    "      available. Indexed as w[beneficiary_idx, state] with state=0 => NE state.\n",
    "  available_arms: An array of size N with binary values {0,1}, where 0 denotes\\\n",
    "      sleeping arm and 1 denotes available arm. \n",
    "  sleeping_constraint: Boolean indicating whether to impose frequency constraint\n",
    "  \n",
    "  Outputs:\n",
    "    actions: A vector of size N with binary values {0,1} representing action \\\n",
    "    chosen for each beneficiary  \n",
    "  '''\n",
    "  SOME_LARGE_NUMBER=10000\n",
    "  N= states.shape[0]\n",
    "  if policy==0: \n",
    "    ## No interventions\n",
    "    return np.zeros(N)\n",
    "  if policy==1: \n",
    "    ## Intervene on everybody\n",
    "    if sleeping_constraint:\n",
    "      return np.logical_and(np.ones(N), available_arms)+0   # with constraint\n",
    "    else:\n",
    "      return np.ones(N)                                     # without constraint\n",
    "  if policy==2:\n",
    "    ## select k arms at random\n",
    "    actions=np.zeros(N)\n",
    "    if sleeping_constraint:\n",
    "      actions[np.random.choice(np.where(available_arms)[0],                    \\\n",
    "                          size=min(k,np.sum(available_arms[available_arms>0])),\\\n",
    "                          replace=False)]=1         # with constraint\n",
    "    else:\n",
    "      actions[np.random.choice(np.arange(N), size=k, replace=False)]=1         \\\n",
    "                                                    # without constraint\n",
    "    return actions\n",
    "  if policy==3:\n",
    "    ## select k arms in a round robin fashion\n",
    "    actions=np.zeros(N)\n",
    "    if sleeping_constraint:\n",
    "      if np.sum(available_arms)<=k:\n",
    "        actions[np.where(available_arms)[0]]=1\n",
    "      else:\n",
    "        actions[[(timestep*k+i)%N for i in range(k)]]=1\n",
    "        '''This is assuming that because more arms are available than required,\\\n",
    "        the arms that we would have pulled otherwise would always be available'''\n",
    "    else:\n",
    "      actions[[(timestep*k+i)%N for i in range(k)]]=1\n",
    "    return actions\n",
    "\n",
    "  if policy==4:\n",
    "    ## select k arms maximizing myopic rewards\n",
    "    actions=np.zeros(N)\n",
    "    myopic_rewards=T[np.arange(N), 1, states, 1]-T[np.arange(N), 0, states, 1]\n",
    "    if sleeping_constraint:\n",
    "      myopic_rewards[np.where(1-available_arms)]=-SOME_LARGE_NUMBER\n",
    "      ## This is preclude sleeping arms from possibility of selection. \n",
    "      ## Setting it to -infty can lead to numerical errors\n",
    "      top_k_myopic_rewards=getTopk(myopic_rewards, \\\n",
    "                                   min(k, int(np.sum(available_arms))))\n",
    "    else:\n",
    "      top_k_myopic_rewards=getTopk(myopic_rewards, k)\n",
    "    top_k_positive_myopic_rewards=np.array([idx for idx in top_k_myopic_rewards\\\n",
    "                                            if myopic_rewards[idx]>0])\n",
    "    ## Pull only arms with positive mypoic value\n",
    "    if len(top_k_positive_myopic_rewards)>0:\n",
    "      actions[top_k_positive_myopic_rewards]=1\n",
    "    return actions\n",
    "\n",
    "  if policy==4.5:\n",
    "    ## select k arms with highest active transition probs\n",
    "    actions=np.zeros(N)\n",
    "    myopic_rewards=T[np.arange(N), 1, states, 1]\n",
    "    if sleeping_constraint:\n",
    "      myopic_rewards[np.where(1-available_arms)]=-SOME_LARGE_NUMBER\n",
    "      ## This is preclude sleeping arms from possibility of selection. \n",
    "      ## Setting it to -infty can lead to numerical errors\n",
    "      top_k_myopic_rewards=getTopk(myopic_rewards, \\\n",
    "                                   min(k, int(np.sum(available_arms))))\n",
    "    else:\n",
    "      top_k_myopic_rewards=getTopk(myopic_rewards, k)\n",
    "    top_k_positive_myopic_rewards=np.array([idx for idx in top_k_myopic_rewards\\\n",
    "                                            if myopic_rewards[idx]>0])\n",
    "    ## Pull only arms with positive mypoic value\n",
    "    if len(top_k_positive_myopic_rewards)>0:\n",
    "      actions[top_k_positive_myopic_rewards]=1\n",
    "    return actions\n",
    "\n",
    "  if policy==5:\n",
    "    ## select k arms according to whittle indices\n",
    "    actions=np.zeros(N)\n",
    "    whittle_indices=w[np.arange(N), states]\n",
    "    if sleeping_constraint:\n",
    "      whittle_indices[np.where(1-available_arms)]=-SOME_LARGE_NUMBER \n",
    "      ## This is preclude sleeping arms from possibility of selection. \n",
    "      ## Setting it to -infty can lead to numerical errors\n",
    "      top_k_whittle=getTopk(whittle_indices, min(k,int(np.sum(available_arms))))\n",
    "    else:\n",
    "      top_k_whittle=getTopk(whittle_indices, k)\n",
    "    top_k_positive_whittle=np.array([idx for idx in top_k_whittle if \\\n",
    "                                     whittle_indices[idx]>0])\n",
    "    ## Pull only arms with positive whittle indices\n",
    "    if len(top_k_positive_whittle)>0:\n",
    "      actions[top_k_positive_whittle]=1\n",
    "    return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "yBlgZW3bqHGq"
   },
   "outputs": [],
   "source": [
    "def runSimulation(args, T=None,w=None, sleeping_constraint=True,\\\n",
    "                  counterExample=False, badf=50):\n",
    "  '''\n",
    "  Runs the simulation for the given set of input arguments and returns rewards\n",
    "\n",
    "  Inputs: \n",
    "    args: All simulations settings\n",
    "    T: Transition matrix if to be used from existing data\n",
    "    w: Whittle indices corresponding to transition matrix\n",
    "    sleeping_constraint: If True, imposes frequency constraint\n",
    "    counterExample: If True and if T is None, then generates T matrix according\n",
    "                    to adversarial example\n",
    "    badf: Percentage of non-recoverable beneficiaries in counterExample\n",
    "\n",
    "  Outputs: \n",
    "    simulated_rewards: Matrix of size num_trials X num_policies. Each entry is \n",
    "                      total raw reward of a policy for that trial\n",
    "    state_record: Engagement matrix of size num_trials X policies X N X L \\\n",
    "                indexed as [trial no.][policy][beneficiary idx][timestep]\n",
    "  '''\n",
    "  E_START_STATE_PROB=0.95\n",
    "  ##### Unpack arguments\n",
    "  L=args.simulation_length\n",
    "  N=args.num_beneficiaries\n",
    "  k=args.num_resources\n",
    "  ntr=args.num_trials\n",
    "  if args.policy<0:\n",
    "    policies=[0,1,2,3,4,4.5,5]\n",
    "  else:\n",
    "    policies=[args.policy]\n",
    "  policy_names={0:'Nobody',1:'Everybody', 2:'Random', 3:'RR', \\\n",
    "                4:'Myopic (Greedy Active-Passive)', 4.5: 'Greedy Active Only', 5:'Whittle'}\n",
    "  state_record=np.zeros((ntr, len(policies),N,L))  # Store the full state\n",
    "                                                   # trajectory\n",
    "  simulated_rewards=np.zeros((ntr, len(policies))) # Store aggregate rewards\n",
    "  \n",
    "  ##### Iterate over number of independent trials to average over\n",
    "  for tr in range(ntr):\n",
    "    ## Initialize for each trial\n",
    "    np.random.seed(seed=tr+args.seed_base)\n",
    "    \n",
    "    states=np.random.binomial(1, E_START_STATE_PROB, size=N) # Random state \\\n",
    "                                                             #initialization                      \n",
    "    valid_matrix=False\n",
    "    ## Generate T matrix if None\n",
    "    if T is None:\n",
    "      T=np.zeros((N,2,2,2))\n",
    "      for i in range(N):\n",
    "        valid_matrix=False\n",
    "        while not valid_matrix:\n",
    "          if counterExample: \n",
    "            ## Counter Example matrix\n",
    "            Ti=getCounterExampleTmatrix(1, badf)\n",
    "          else: \n",
    "            ## Randomly sampled matrix\n",
    "            Ti=generateRandomTmatrix(1)\n",
    "          valid_matrix=verify_T_matrix(Ti[0])\n",
    "        T[i]=Ti[0]\n",
    "\n",
    "    ## Generate whittle index matrix if None\n",
    "    if w is None: \n",
    "      w=np.zeros((N,2))\n",
    "      for i in tqdm.tqdm(range(N)):\n",
    "        w[i,:]=np.array(getWhittleIndex(T[i], \\\n",
    "        sleeping_constraint=sleeping_constraint))\n",
    "    \n",
    "    ## For current trial, evaluate all policies\n",
    "    for pol_idx, pol in enumerate(policies):\n",
    "      ## Iterate over timesteps. Note that if simulation length is L, \n",
    "      ## there are L-1 action decisions to take.\n",
    "      previous_three_actions=[[],[],[]]          # for tracking sleeping arms\n",
    "      for timestep in range(L-1):\n",
    "        ## Put previously pulled arms to sleep\n",
    "        available_arms=np.ones_like(states)\n",
    "        for prev_a in previous_three_actions:\n",
    "          for arm_idx in prev_a:\n",
    "            available_arms[arm_idx]=0\n",
    "        \n",
    "        ## Compute actions\n",
    "        state_record[tr, pol_idx, :, timestep]=states\n",
    "        actions=getActions(states, T, k, pol, timestep=timestep, w=w,\\\n",
    "                           available_arms=available_arms, \\\n",
    "                           sleeping_constraint=sleeping_constraint)\n",
    "        states= takeActions(states, T, actions)\n",
    "        \n",
    "        ## Update prev_actions array\n",
    "        previous_three_actions[timestep%3]=np.where(actions)[0]\n",
    "      \n",
    "      simulated_rewards[tr, pol_idx]=np.mean(np.sum(state_record[tr,pol_idx], \\\n",
    "                                                    axis=1))\n",
    "  \n",
    "  ##### Print results and plot the output\n",
    "  for pol in policies: \n",
    "    print (\"Expected reward of policy %s is %s\"%(policy_names[pol], \\\n",
    "                            np.mean(simulated_rewards[:,policies.index(pol)])))\n",
    "  \n",
    "  return simulated_rewards, state_record\n",
    "  \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_list = {'rmab': [1, 125, 200, 200, 200, 235, 300, 350, 500, 500], 'round_robin': [1, 125, 200, 200, 200, 200, 300, 350, 500, 500], 'control': [1]*10}\n",
    "call_list_cumulative = {'rmab': [1,125,325,525,725,960,1260,1610,2110,2610, 3110], 'round_robin': [1,125,325,525,725,925,1225,1575,2075,2575, 3075], 'control': [1]*10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ONTafk3bN5W3"
   },
   "outputs": [],
   "source": [
    "def runRealDataExp(args):\n",
    "    '''\n",
    "    Runs experiments for generating simulated outcomes using transition \\\n",
    "    probabilities estimated from real data\n",
    "    '''\n",
    "    #### Settings \n",
    "#     args.num_resources=1530\n",
    "#     args.simulation_length=40\n",
    "#     args.num_trials=30\n",
    "    simulated_rewards, state_record= runSimulation(args,T= T_data , w=w)\n",
    "    policy_names={0:'Nobody',1:'Everybody', 2:'Random', 3:'Round Robin', \\\n",
    "                  4:'Greedy Active-Passive', 4.5: 'Greedy Active Only',\n",
    "                  5:'Whittle'}\n",
    "    policies=[0,1,2,3,4, 4.5, 5]\n",
    "    plotIB(simulated_rewards, policies, policy_names, \\\n",
    "           policies_to_plot=[2,3,4,4.5, 5], upper_pol=3)\n",
    "\n",
    "\n",
    "def runCounterExample(args):\n",
    "    \n",
    "    #### Settings     \n",
    "    args.num_resources=30\n",
    "    args.simulation_length=200\n",
    "    args.num_trials=30\n",
    "    args.num_beneficiaries=100\n",
    "    policies_to_plot=[5,4,3,2]\n",
    "    policies=[0,1,2,3,4,5]\n",
    "    policy_names={0:'Nobody',1:'Everybody', 2:'Random', 3:'Round Robin', \\\n",
    "                  4:'Myopic', 5:'Whittle'}\n",
    "    N=args.num_beneficiaries\n",
    "    simulated_rewards={}\n",
    "    state_record={}\n",
    "    ibs={}\n",
    "    errors_ibs={}\n",
    "    badfs= [10,20,30,40,50,60,70,80,90] \n",
    "\n",
    "    for badf in badfs:\n",
    "      simulated_rewards[badf], state_record[badf]= \\\n",
    "      runSimulation(args,T= None , w=None,  sleeping_constraint=False, \\\n",
    "                    counterExample=True, badf=badf)\n",
    "      \n",
    "      ibs[badf], errors_ibs[badf]=plotIB(simulated_rewards[badf], policies,\\\n",
    "                                         policy_names, policies_to_plot=\\\n",
    "                                         [2,3,4,5], upper_pol=5)\n",
    "                      \n",
    "    ### Dump as pickle to avoid running it again in case needed\n",
    "    with open('counterExample.pickle', 'wb') as f:\n",
    "      pickle.dump([ibs, errors_ibs,  simulated_rewards, state_record], f)\n",
    "\n",
    "    fig=plt.figure(figsize=(6,4))\n",
    "    for pol in policies_to_plot[:]:\n",
    "      y=np.array([ibs[badf][pol] for badf in badfs])\n",
    "      error=np.array([errors_ibs[badf][pol] for badf in badfs])\n",
    "      #plt.errorbar(badfs, y, yerr=error, capsize=4, label=policy_names[pol])\n",
    "      plt.plot(badfs, y, label=policy_names[pol])\n",
    "      plt.fill_between(badfs, y-error, y+error, alpha=0.2)\n",
    "    plt.xlabel(\"% of non-recoverable beneficiaries\", fontsize=16)\n",
    "    plt.ylabel(\"Intervention Benefit (%)\", fontsize=18)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def runResourcesExp(args, data='real'):\n",
    "    '''\n",
    "    data: 'real': Run it on pilot test data\n",
    "          'random': Generate random T matrix\n",
    "    '''\n",
    "\n",
    "    resource_percents=[5., 10., 15., 20., 25., 30., 35., 40.]\n",
    "    policy_names={0:'Nobody',1:'Everybody', 2:'Random', 3:'Round Robin', \\\n",
    "                  4:'Myopic', 5:'Whittle'}\n",
    "    policies=[0,1,2,3,4,5]\n",
    "    policies_to_plot=[1,5,4,3,2,0]\n",
    "    if data=='random':\n",
    "      args.num_beneficiaries=100\n",
    "      \n",
    "    simulated_rewards={}\n",
    "    state_record={}\n",
    "    simulated_rewards_no_constraint={}\n",
    "    state_record_no_constraint={}\n",
    "    for k in resource_percents:\n",
    "      print('Computing for k=%s'%(k))\n",
    "      args.num_resources=int((k*args.num_beneficiaries)/100.)\n",
    "      args.simulation_length=40\n",
    "      args.num_trials=50\n",
    "      if data=='real':\n",
    "        simulated_rewards[k], state_record[k]= \\\n",
    "        runSimulation(args,T= T_data , w=w)\n",
    "        simulated_rewards_no_constraint[k], state_record_no_constraint[k]= \\\n",
    "        runSimulation(args,T= T_data , w=w, sleeping_constraint=False)\n",
    "      elif data=='random':\n",
    "        simulated_rewards[k], state_record[k]=\\\n",
    "        runSimulation(args,T= None , w=None)\n",
    "        simulated_rewards_no_constraint[k], state_record_no_constraint[k]= \\\n",
    "        runSimulation(args,T= None , w=None, sleeping_constraint=False)\n",
    "        \n",
    "\n",
    "    fig=plt.figure(figsize=(6,4))\n",
    "    for pol in policies_to_plot[:]:\n",
    "      y=np.array([args.num_beneficiaries*np.mean(simulated_rewards[k][:, pol]) \\\n",
    "                  for k in resource_percents])\n",
    "      error=np.array([args.num_beneficiaries*np.std(simulated_rewards[k][:, pol])\\\n",
    "                      /np.sqrt(simulated_rewards[k].shape[0]) \\\n",
    "                      for k in resource_percents])\n",
    "      #plt.errorbar(resource_percents, y, yerr=error, capsize=4, \\\n",
    "      #label=policy_names[pol])\n",
    "      plt.plot(resource_percents, y, label=policy_names[pol])\n",
    "      plt.fill_between(resource_percents, y-error, y+error, alpha=0.2)\n",
    "    plt.xlabel(\"Intervention resources available as % of total beneficiaries\", \\\n",
    "               fontsize=16)\n",
    "    plt.ylabel(\"Total engagements\", fontsize=18)\n",
    "    plt.title(\"With frequency constraint\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    fig=plt.figure(figsize=(6,4))\n",
    "    for pol in policies_to_plot[1:]:\n",
    "      y=[args.num_beneficiaries*np.mean(\\\n",
    "                                  simulated_rewards_no_constraint[k][:, pol])\\\n",
    "                                 for k in resource_percents]\n",
    "      error=[args.num_beneficiaries*\\\n",
    "             np.std(simulated_rewards_no_constraint[k][:, pol])\\\n",
    "             /np.sqrt(simulated_rewards_no_constraint[k].shape[0])\\\n",
    "             for k in resource_percents]\n",
    "      plt.errorbar(resource_percents, y, yerr=error, capsize=4, \\\n",
    "                   label=policy_names[pol])\n",
    "    plt.xlabel(\"Intervention resources available as % of total beneficiaries\",\\\n",
    "               fontsize=16)\n",
    "    plt.ylabel(\"Total engagements\", fontsize=16)\n",
    "    plt.title(\"Without frequency constraint\")\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "def makeEngagementPlots(PLOT_OPTION=0, T=10):\n",
    "  \"\"\"\n",
    "  Generates engagement plots. \n",
    "  INputs: \n",
    "    PLOT_OPTION: Which kind of plot to generate. See comments for description\n",
    "    T: Number of weeks till which plots to be made\n",
    "  \"\"\"\n",
    "  #### Line plot for engagament patterns\n",
    "  \n",
    "  x=np.arange(T)\n",
    "  y={}\n",
    "  y_err={}\n",
    "  y_drop_cumulative={}\n",
    "  for pol in ['rmab', 'round_robin', 'control']:\n",
    "    y[pol]=[]\n",
    "    y_err[pol]=[]\n",
    "    y_drop_cumulative[pol]=[]\n",
    "    baseline_for_this_pol=np.mean(engagement_matrix[pol][:,0])\n",
    "    baseline_for_this_pol_raw=np.sum(engagement_matrix[pol][:,0])\n",
    "    num_trials=engagement_matrix[pol].shape[0]\n",
    "    for t in range(T):\n",
    "      if PLOT_OPTION==0:\n",
    "        ## Cumulative drop till week x measured as % of population\n",
    "        y[pol].append(100*(np.mean(np.sum(engagement_matrix[pol][:,:t+1], \\\n",
    "                                        axis=1))- (t+1)*baseline_for_this_pol))     \n",
    "        y_err[pol].append(100*np.std(np.sum(engagement_matrix[pol][:,:t+1], \\\n",
    "                              axis=1))/np.sqrt(engagement_matrix[pol].shape[0]))\n",
    "        ylabel='Cumulative drop in engagement rate'\n",
    "      elif PLOT_OPTION==1:\n",
    "        ## Instataneous drop at week x compared to week0 measured as % of total\n",
    "        y[pol].append(100*(np.mean(np.sum(engagement_matrix[pol][:,t:t+1], \\\n",
    "                                          axis=1))- baseline_for_this_pol))          \n",
    "        y_err[pol].append(100*np.std(np.sum(engagement_matrix[pol][:,t:t+1], \\\n",
    "                              axis=1))/np.sqrt(engagement_matrix[pol].shape[0]))\n",
    "        ylabel='Current drop in engagement rate'\n",
    "      elif PLOT_OPTION==2:\n",
    "        ## Absolute cumulative engagement rate till week x\n",
    "        y[pol].append(np.mean(np.sum(engagement_matrix[pol][:,:t+1], axis=1)))                                  \n",
    "        y_err[pol].append(np.std(np.sum(engagement_matrix[pol][:,:t+1], \\\n",
    "                          axis=1))/(np.sqrt(engagement_matrix[pol].shape[0])))\n",
    "        ylabel='Cumulative engagements per beneficiary'\n",
    "      elif PLOT_OPTION==3:\n",
    "        ## Absolute cumulative average (per-week) engagement rate till week x\n",
    "        y[pol].append(np.sum(np.sum(engagement_matrix[pol][:,:t+1], axis=1))\\\n",
    "                      /(t+1))\n",
    "        y_err[pol].append(np.std(np.sum(engagement_matrix[pol][:,:t+1], \\\n",
    "                    axis=1))/(np.sqrt(engagement_matrix[pol].shape[0])*(t+1)))\n",
    "        ylabel='Average Cumulative engagement rate'                                  \n",
    "      elif PLOT_OPTION==4:\n",
    "        ## Absolute engagement rate at week x\n",
    "        y[pol].append(np.sum(np.sum(engagement_matrix[pol][:,t:t+1], axis=1)))                                  \n",
    "        y_err[pol].append(np.std(np.sum(engagement_matrix[pol][:,t:t+1], \\\n",
    "                          axis=1))/(np.sqrt(engagement_matrix[pol].shape[0])))\n",
    "        ylabel='Current engagement rate'\n",
    "      \n",
    "      ## Cumulative Drop values\n",
    "      y_drop_cumulative[pol].append(np.sum(np.sum(engagement_matrix[pol]\\\n",
    "                          [:,t:t+1], axis=1))- (1)*baseline_for_this_pol_raw)     \n",
    "  #### Plotting function\n",
    "  pol_labels={'rmab':'RMAB group', 'round_robin':'Round Robin', \\\n",
    "              'control':'Control group'}\n",
    "  if PLOT_OPTION == 0:\n",
    "    for pol in ['rmab', 'round_robin', 'control']:\n",
    "      y[pol] = np.array(y[pol])\n",
    "    for pol in ['rmab', 'round_robin']:\n",
    "      y[pol] = y[pol] / y['control']\n",
    "      y[pol] = np.nan_to_num(y[pol], nan=1.0)\n",
    "    y['control'] = np.array([1]*y['control'].shape[0])\n",
    "  fig=plt.figure(figsize=(6,4))\n",
    "  for pol in ['rmab', 'round_robin', 'control']:\n",
    "    #plt.errorbar(x,y[pol], yerr=y_err[pol],  ls=':', capsize=4, label=pol[:])\n",
    "    plt.plot(x,y[pol], 'o-', label=pol_labels[pol] )\n",
    "    print (\"Policy:\", pol, \" value:\", y[pol])\n",
    "#     plt.fill_between(x, np.array(y[pol])-np.array(y_err[pol]), \\\n",
    "#                      np.array(y[pol])+np.array(y_err[pol]), alpha=0.2)\n",
    "  plt.legend(fontsize=14)\n",
    "  plt.xlabel('Weeks', fontsize=18)\n",
    "  plt.ylabel(ylabel, fontsize=14)\n",
    "  #plt.yticks(ticks=[0,-5,-10,-15,-20], \\\n",
    "  #labels=['0%','--5%','--10%','--15%','--20%'])\n",
    "  #plt.yticks(ticks=[0,-1,-2,-3,-4,-5], \\\n",
    "  #labels=['0%','--1%','--2%','--3%','--4%','--5%'])\n",
    "  plt.tight_layout() \n",
    "  #plt.savefig('change-in-engagement.png')\n",
    "  plt.show()\n",
    "  fig=plt.figure(figsize=(6,4))\n",
    "  for pol in ['rmab', 'round_robin', 'control']:\n",
    "    #plt.errorbar(x,[y[pol][i]-y['control'][i] for i in range(7)], \\\n",
    "    #yerr=[y_err[pol][i] +y_err['control'][i] for i in range(7)], \\\n",
    "    #capsize=4, label=pol[:])\n",
    "    plt.plot(x,[(y_drop_cumulative[pol][i]-y_drop_cumulative['control'][i]) / (call_list[pol][i]) \\\n",
    "                for i in range(T)], '-^', label=pol_labels[pol])\n",
    "    print (\"Policy:\", pol, \" value:\",\\\n",
    "           [y_drop_cumulative[pol][i]-y_drop_cumulative['control'][i] \\\n",
    "            for i in range(T)])\n",
    "    #plt.fill_between(x, np.array(y[pol])-np.array(y['control'])-\\\n",
    "    #np.array(y_err[pol])-np.array(y_err['control']), np.array(y[pol])-\\\n",
    "    #np.array(y['control'])+np.array(y_err[pol])+np.array(y_err['control']), \\\n",
    "    #alpha=0.2)\n",
    "  plt.legend(fontsize=14)\n",
    "  plt.xlabel('Weeks', fontsize=18)\n",
    "  plt.ylabel('Engagement Drop Prevented', fontsize=16)\n",
    "  plt.tight_layout() \n",
    "  #plt.savefig('drop-prevented.png')\n",
    "  plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1623909242055,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "SlnU_-0URHxY",
    "outputId": "cb9b6bd8-8d99-4512-accd-25cbc7850e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected reward of policy Nobody is 12.05577725612937\n",
      "Expected reward of policy Everybody is 13.249895670318205\n",
      "Expected reward of policy Random is 12.965727699530518\n",
      "Expected reward of policy RR is 12.861789254042776\n",
      "Expected reward of policy Myopic (Greedy Active-Passive) is 13.526043296817942\n",
      "Expected reward of policy Greedy Active Only is 13.256794470526865\n",
      "Expected reward of policy Whittle is 13.643022952529995\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Inputs to engagement simulator module')\n",
    "    parser.add_argument('-N', '--num_beneficiaries', default=7000, type=int, help='Number of Beneficiaries')\n",
    "    parser.add_argument('-k', '--num_resources', default=1400, type=int, help='Number of calls available per day')\n",
    "    parser.add_argument('-L', '--simulation_length', default=40, type=int, help='Number of timesteps of simulation')\n",
    "    parser.add_argument('-ntr', '--num_trials', default=10, type=int, help='Number of independent trials')\n",
    "    parser.add_argument('-s', '--seed_base', default=0, type=int, help='Seedbase for numpy. This is starting seedbase. Simulation will consider the seeds= {seed_base, ... seed_base+ntr-1}')\n",
    "    parser.add_argument('-p', '--policy', default=-1, type=int, help='policy to run. default is all policies')\n",
    "    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    #T_data,w= loadBeneficiaryData()\n",
    "    args.num_beneficiaries=T_data.shape[0]\n",
    "#     args.simulation_length=20\n",
    "#     args.num_resources=2500\n",
    "    \n",
    "    args.num_resources=1500\n",
    "    args.simulation_length=50\n",
    "    args.num_trials=10\n",
    "    \n",
    "    runRealDataExp(args)\n",
    "#     runResourcesExp(args)\n",
    "    #runResourcesExp(args, data='random')\n",
    "    #runCounterExample(args)\n",
    "#     makeEngagementPlots(PLOT_OPTION=0)\n",
    "#     makeEngagementPlots(PLOT_OPTION=1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKtL4p9o6Ez-"
   },
   "source": [
    "**TEST CODE/UNIT TESTS BELOW**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1622521752844,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "BZpx1KHj1AHk",
    "outputId": "b20f5350-0f74-4d5e-d31e-88c1060c868f"
   },
   "outputs": [],
   "source": [
    "T_swapactions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rk51-ajRStv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1622510062241,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "ZfosM_Kk2I-E",
    "outputId": "db8a996c-7d8d-4bdf-8fba-716e86e06ece"
   },
   "outputs": [],
   "source": [
    "'''Test code to test out takeActions function'''\n",
    "states=np.array([0,0,1,1,1])\n",
    "actions=np.array([0,1,1,1,1])\n",
    "T1=[[[0.7, 0.1],[0.4,0.6]], [[0.5, 0.5],[0.1,0.95]]]\n",
    "T2=[[[0.7, 0.15],[0.4,0.6]], [[0.5, 0.5],[0.1,0.96]]]\n",
    "T3=[[[0.7, 0.2],[0.4,0.6]], [[0.5, 0.5],[0.1,0.97]]]\n",
    "T=np.array([T1,T2,T3,T2,T1])\n",
    "for i in range(10):\n",
    "  #print(takeActions(states, T, actions))\n",
    "  print(getActions(states, T, 3, policy=4, timestep=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkila8JxbdgN"
   },
   "outputs": [],
   "source": [
    "rmab_group_results = pd.read_csv('rmab_pilot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1621877844478,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "Xd5Lz-WhbonE",
    "outputId": "ca040bcb-565b-4351-8b0d-50e4fc97d187"
   },
   "outputs": [],
   "source": [
    "rmab_group_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1622509975541,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "07-c70eiYutk",
    "outputId": "af095e3b-737a-4993-ef93-8b1550586890"
   },
   "outputs": [],
   "source": [
    "#cd 'drive/My\\ Drive/ARMMAN/code/'\n",
    "%cd drive/My\\ Drive/ARMMAN/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1623362578099,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "_mm2A9MpwqUW",
    "outputId": "2de6f8f3-b993-4819-fd32-a3c67b90e125"
   },
   "outputs": [],
   "source": [
    "engagement_matrix.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PitJRCnjc-d_"
   },
   "outputs": [],
   "source": [
    "actions=np.array([0,0,0,0,1,1])\n",
    "np.where(actions)[0]\n",
    "b= np.logical_and(np.ones_like(actions), np.array([0,0,1,1,1,1]))+0\n",
    "#print(np.sum(actions[actions>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1622957287828,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "vS3sGvaT5nXi",
    "outputId": "0fcb2cdf-1a29-49c2-9e5c-2921de16af1d"
   },
   "outputs": [],
   "source": [
    "print (b)\n",
    "b[np.where((1-actions))[0]]=-100\n",
    "print(b[np.where((1-actions))[0]])\n",
    "print (b)\n",
    "np.where((1-actions))[0]\n",
    "bk=getTopk(b, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2iQ7Hu8L64z"
   },
   "outputs": [],
   "source": [
    "def get_reward(state, action, m):\n",
    "    if state[0] == \"L\":\n",
    "        reward = 1.0\n",
    "    else:\n",
    "        reward = -1.0\n",
    "    if action == 'N':\n",
    "        reward += m\n",
    "\n",
    "    return reward\n",
    "\n",
    "def convertAxis(T):\n",
    "    '''\n",
    "    conver T matrix from format: a, s, s' (where s=0 is bad state) --> s, s', a (where s=0 is good state) \n",
    "    '''\n",
    "    P=np.zeros_like(T)\n",
    "    for a in range(2):\n",
    "      for s in range(2):\n",
    "        for ss in range(2):\n",
    "          P[1-s,1-ss,a]=T[a,s,ss]\n",
    "    return P\n",
    "\n",
    "def plan2(two_state_probs, sleeping_constraint = True ):\n",
    "    '''\n",
    "    two_state_probs axes: action, starting_state, final_state. State=1 means engaging state\n",
    "    '''\n",
    "    two_state_probs=convertAxis(two_state_probs)\n",
    "\n",
    "    aug_states = []\n",
    "    for i in range(6):\n",
    "        if i % 2 == 0:\n",
    "            aug_states.append('L{}'.format(i // 2))\n",
    "        else:\n",
    "            aug_states.append('H{}'.format(i // 2))\n",
    "\n",
    "    if sleeping_constraint:\n",
    "        local_CONFIG = {\n",
    "            'problem': {\n",
    "                \"orig_states\": ['L', 'H'],\n",
    "                \"states\": aug_states + ['L', 'H'],\n",
    "                \"actions\": [\"N\", \"I\"],\n",
    "            },\n",
    "            \"time_step\": 7,\n",
    "            \"gamma\": 0.99,\n",
    "        }\n",
    "    else:\n",
    "        local_CONFIG = {\n",
    "            'problem': {\n",
    "                \"orig_states\": ['L', 'H'],\n",
    "                \"states\": ['L', 'H'],\n",
    "                \"actions\": [\"N\", \"I\"],\n",
    "            },\n",
    "            \"time_step\": 7,\n",
    "            \"gamma\": 0.99,\n",
    "        }\n",
    "\n",
    "    v_values = np.zeros(len(local_CONFIG['problem']['states']))\n",
    "    q_values = np.zeros((len(local_CONFIG['problem']['states']), len(local_CONFIG['problem']['actions'])))\n",
    "    high_m_values = 1 * np.ones(len(local_CONFIG['problem']['states']))\n",
    "    low_m_values = -1 * np.ones(len(local_CONFIG['problem']['states']))\n",
    "\n",
    "    t_probs = np.zeros((len(local_CONFIG['problem']['states']), len(local_CONFIG['problem']['states']), len(local_CONFIG['problem']['actions'])))\n",
    "\n",
    "    if sleeping_constraint:    \n",
    "        t_probs[0 : 2, 2 : 4, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[2 : 4, 4 : 6, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[4 : 6, 6 : 8, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[6 : 8, 6 : 8, 0] = two_state_probs[:, :, 0]\n",
    "\n",
    "        t_probs[0 : 2, 2 : 4, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[2 : 4, 4 : 6, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[4 : 6, 6 : 8, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[6 : 8, 0 : 2, 1] = two_state_probs[:, :, 1]\n",
    "    else:\n",
    "        t_probs = two_state_probs\n",
    "\n",
    "    max_q_diff = np.inf\n",
    "    prev_m_values, m_values = None, None\n",
    "    while max_q_diff > 1e-5:\n",
    "        prev_m_values = m_values\n",
    "        m_values = (low_m_values + high_m_values) / 2\n",
    "        if type(prev_m_values) != type(None) and abs(prev_m_values - m_values).max() < 1e-20:\n",
    "            break\n",
    "        max_q_diff = 0\n",
    "        v_values = np.zeros((len(local_CONFIG['problem']['states'])))\n",
    "        q_values = np.zeros((len(local_CONFIG['problem']['states']), len(local_CONFIG['problem']['actions'])))\n",
    "        delta = np.inf\n",
    "        while delta > 0.0001:\n",
    "            delta = 0\n",
    "            for i in range(t_probs.shape[0]):\n",
    "                v = v_values[i]\n",
    "                v_a = np.zeros((t_probs.shape[2],))\n",
    "                for k in range(v_a.shape[0]):\n",
    "                    for j in range(t_probs.shape[1]):\n",
    "                        v_a[k] += t_probs[i, j, k] * (get_reward(local_CONFIG['problem']['states'][i], local_CONFIG['problem']['actions'][k], m_values[i]) + local_CONFIG[\"gamma\"] * v_values[j])\n",
    "\n",
    "                v_values[i] = np.max(v_a)\n",
    "                delta = max([delta, abs(v_values[i] - v)])\n",
    "\n",
    "        state_idx = -1\n",
    "        for state in range(q_values.shape[0]):\n",
    "            for action in range(q_values.shape[1]):\n",
    "                for next_state in range(q_values.shape[0]):\n",
    "                    q_values[state, action] += t_probs[state, next_state, action] * (get_reward(local_CONFIG['problem']['states'][state], local_CONFIG['problem']['actions'][action], m_values[state]) + local_CONFIG[\"gamma\"] * v_values[next_state])\n",
    "            # print(state, q_values[cluster, state, 0], q_values[cluster, state, 1])\n",
    "\n",
    "        for state in range(q_values.shape[0]):\n",
    "            if abs(q_values[state, 1] - q_values[state, 0]) > max_q_diff:\n",
    "                state_idx = state\n",
    "                max_q_diff = abs(q_values[state, 1] - q_values[state, 0])\n",
    "\n",
    "        # print(q_values)\n",
    "        # print(low_m_values, high_m_values)\n",
    "        if max_q_diff > 1e-5 and q_values[state_idx, 0] < q_values[state_idx, 1]:\n",
    "            low_m_values[state_idx] = m_values[state_idx]\n",
    "        elif max_q_diff > 1e-5 and q_values[state_idx, 0] > q_values[state_idx, 1]:\n",
    "            high_m_values[state_idx] = m_values[state_idx]\n",
    "\n",
    "        # print(low_m_values, high_m_values, state_idx)\n",
    "        # ipdb.set_trace()\n",
    "    \n",
    "    m_values = (low_m_values + high_m_values) / 2\n",
    "\n",
    "    #return q_values, m_values\n",
    "    return [m_values[-1], m_values[-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4262,
     "status": "ok",
     "timestamp": 1623384002211,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "hG4rHbepMkMx",
    "outputId": "81aa6989-637b-4b41-9692-d610eab5478f"
   },
   "outputs": [],
   "source": [
    "T=np.zeros((1,2,2,2))\n",
    "N=1\n",
    "valid_matrix=False\n",
    "for i in range(N):\n",
    "  while not valid_matrix:\n",
    "    Ti=generateRandomTmatrix(1)\n",
    "    #valid_matrix=verify_T_matrix(Ti[0])\n",
    "    valid_matrix=True\n",
    "  T[i]=Ti[0]\n",
    "plan2(T[0], sleeping_constraint = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iV3032w9MwlV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34598,
     "status": "ok",
     "timestamp": 1623386569863,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "OgWHDf-GZVMY",
    "outputId": "79dcf7ae-0664-4f47-dda7-420c024675b0"
   },
   "outputs": [],
   "source": [
    "rmab_group_probs,rmab_group_whittle_indices, engagement_matrix= loadBeneficiaryData()\n",
    "rmab_group_probs.shape\n",
    "rmab_group_whittle_indices.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8724,
     "status": "ok",
     "timestamp": 1623387233469,
     "user": {
      "displayName": "Aditya Mate",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gieg2LY0l3jg1ZG71DxFJFZiOjxpaCm3f_5l_-5=s64",
      "userId": "01283220110827855464"
     },
     "user_tz": 240
    },
    "id": "0oBB2VuIttp9",
    "outputId": "46a04521-bbd5-4fb3-975d-c0b0ef7ea15f"
   },
   "outputs": [],
   "source": [
    "print(rmab_group_probs[0,:,:,:])\n",
    "print(\"Indices:\", rmab_group_whittle_indices[0])\n",
    "print(\"Computed indices:\", plan2(rmab_group_probs[0], sleeping_constraint = True))\n",
    "#print(rmab_group_probs[0,:,:,0])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "engagement_simulator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
