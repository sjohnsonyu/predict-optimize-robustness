{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7ff3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vermashresth/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (25,48,49,51,53,54,55,56,57) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "2021-10-07 11:19:29,762 - INFO - load_data - Loading data from folder 'may_data'\n",
      "2021-10-07 11:19:30,198 - INFO - load_data - Successfully loaded and cleaned beneficiary and call data.\n",
      "2021-10-07 11:19:30,199 - INFO - load_data - Beneficiary data contains data for 26548 beneficiaries\n",
      "2021-10-07 11:19:30,213 - INFO - load_data - Call data contains 1224245 call records for 26548 beneficiaries\n",
      "/Users/vermashresth/Documents/armman-analysis/training/dataset.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"duration\"] = data[\"duration\"].astype('uint8')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.lib.arraysetops import unique\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "import pickle\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "np.random.seed(1)\n",
    "\n",
    "from training.utils import load_obj, save_obj\n",
    "from training.data import load_data\n",
    "from training.dataset import _preprocess_call_data, preprocess_and_make_dataset\n",
    "\n",
    "from sklearn.cluster import KMeans, OPTICS, SpectralClustering\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from training.modelling.metrics import F1, Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.models import load_model\n",
    "from training.modelling.dataloader import get_train_val_test\n",
    "\n",
    "stats = pd.read_csv(\"may_data/beneficiary_stats_v5.csv\")\n",
    "beneficiary_data = pd.read_csv(\"may_data/beneficiary/AIRegistration-20200501-20200731.csv\")\n",
    "b_data, call_data = load_data(\"may_data\")\n",
    "call_data = _preprocess_call_data(call_data)\n",
    "all_beneficiaries = stats[stats['Group'].isin([\"Google-AI-Control\", \"Google-AI-Calls\"])]\n",
    "\n",
    "# features_dataset = preprocess_and_make_dataset(b_data, call_data)\n",
    "with open('may_data/features_dataset.pkl', 'rb') as fw:\n",
    "    features_dataset = pickle.load(fw)\n",
    "fw.close()\n",
    "# exit()\n",
    "\n",
    "# beneficiary_splits = load_obj(\"may_data/RMAB_one_month/weekly_beneficiary_splits_single_group.pkl\")\n",
    "\n",
    "aug_states = []\n",
    "for i in range(6):\n",
    "    if i % 2 == 0:\n",
    "        aug_states.append('L{}'.format(i // 2))\n",
    "    else:\n",
    "        aug_states.append('H{}'.format(i // 2))\n",
    "\n",
    "CONFIG = {\n",
    "    \"problem\": {\n",
    "        \"orig_states\": ['L', 'H'],\n",
    "        \"states\": aug_states + ['L', 'H'],\n",
    "        \"actions\": [\"N\", \"I\"],\n",
    "    },\n",
    "    \"time_step\": 7,\n",
    "    \"gamma\": 0.99,\n",
    "    \"clusters\": 40,\n",
    "    \"transitions\": \"weekly\",\n",
    "    \"clustering\": 'kmeans',\n",
    "#     \"pilot_start_date\": sys.argv[3],\n",
    "#     \"fixed_beneficiary_set\": sys.argv[4],\n",
    "#     \"calling_list\": sys.argv[5],\n",
    "    \"week\": 'week5'\n",
    "}\n",
    "\n",
    "if CONFIG['transitions'] == 'weekly':\n",
    "    transitions = pd.read_csv(\"may_data/RMAB_one_month/weekly_transitions_SI_single_group.csv\")\n",
    "else:\n",
    "    transitions = pd.read_csv(\"may_data/RMAB_one_month/transitions_SI_single_group.csv\")\n",
    "\n",
    "def kmeans_missing(X, n_clusters, max_iter=10):\n",
    "    n_clusters = CONFIG['clusters']\n",
    "    missing = ~np.isfinite(X)\n",
    "    mu = np.nanmean(X, 0, keepdims=1)\n",
    "    X_hat = np.where(missing, mu, X)\n",
    "\n",
    "    prev_labels = None\n",
    "    for i in range(max_iter):\n",
    "        if CONFIG['clustering'] == 'optics':\n",
    "            cls = OPTICS(min_samples=4, n_jobs=-1)\n",
    "        elif CONFIG['clustering'] == 'kmeans':\n",
    "            cls = KMeans(n_clusters, n_jobs=-1, random_state=0)\n",
    "        elif CONFIG['clustering'] == 'spectral':\n",
    "            cls = SpectralClustering(n_clusters, n_jobs=-1, random_state=0)\n",
    "\n",
    "        labels = cls.fit_predict(X_hat)\n",
    "\n",
    "        if CONFIG['clustering'] == 'kmeans':\n",
    "            centroids = cls.cluster_centers_\n",
    "        else:\n",
    "            if CONFIG['clustering'] == 'optics':\n",
    "                labels = labels + 1\n",
    "            unique_labels = len(set(labels))\n",
    "            centroids = []\n",
    "            for i in range(unique_labels):\n",
    "                idxes = np.where(labels == i)[0]\n",
    "                centroids.append(np.mean(X_hat[idxes], axis=0))\n",
    "            centroids = np.array(centroids)\n",
    "\n",
    "        X_hat[missing] = centroids[labels][missing]\n",
    "\n",
    "        if i > 0 and np.all(labels == prev_labels):\n",
    "            break\n",
    "\n",
    "        prev_labels = labels\n",
    "\n",
    "    return labels, centroids, X_hat, cls, len(set(labels)), i\n",
    "\n",
    "def get_static_feature_clusters(train_beneficiaries, train_transitions, features_dataset, n_clusters):\n",
    "    cols = [\n",
    "        \"P(L, I, L)\", \"P(L, I, H)\", \"P(H, I, L)\", \"P(H, I, H)\", \"P(L, N, L)\", \"P(L, N, H)\", \"P(H, N, L)\", \"P(H, N, H)\", \n",
    "    ]\n",
    "\n",
    "    user_ids, dynamic_xs, gest_ages, static_xs, ngo_hosp_ids, labels = features_dataset\n",
    "    \n",
    "    train_ids = train_beneficiaries['user_id']\n",
    "    idxes = [np.where(user_ids == x)[0][0] for x in train_ids]\n",
    "    train_static_features = static_xs[idxes]\n",
    "    train_static_features = train_static_features[:, : -8]\n",
    "\n",
    "    # test_ids = test_beneficiaries['user_id']\n",
    "    # idxes = [np.where(user_ids == x)[0][0] for x in test_ids]\n",
    "    # test_static_features = static_xs[idxes]\n",
    "    # test_static_features = test_static_features[:, : -8]\n",
    "\n",
    "    train_labels, centroids, _, cls, num_clusters, max_iters = kmeans_missing(train_static_features, n_clusters, max_iter=100)\n",
    "    train_beneficiaries['cluster'] = train_labels\n",
    "    # test_beneficiaries['cluster'] = cls.predict(test_static_features)\n",
    "\n",
    "    cluster_transition_probabilities = pd.DataFrame(columns=['cluster', 'count'] + cols)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        cluster_beneficiaries = train_beneficiaries[train_beneficiaries['cluster'] == i]\n",
    "        cluster_b_user_ids = cluster_beneficiaries['user_id']\n",
    "        probs, _ = get_transition_probabilities(cluster_b_user_ids, train_transitions, min_support=3)\n",
    "        probs['cluster'] = i\n",
    "        probs['count'] = len(cluster_b_user_ids)\n",
    "        cluster_transition_probabilities = cluster_transition_probabilities.append(probs, ignore_index=True)\n",
    "\n",
    "    # ipdb.set_trace()\n",
    "\n",
    "    return cluster_transition_probabilities, cls\n",
    "\n",
    "def get_individual_transition_clusters(train_beneficiaries, train_transitions, features_dataset, n_clusters):\n",
    "    cols = [\n",
    "        \"P(L, I, L)\", \"P(L, I, H)\", \"P(H, I, L)\", \"P(H, I, H)\", \"P(L, N, L)\", \"P(L, N, H)\", \"P(H, N, L)\", \"P(H, N, H)\", \n",
    "    ]\n",
    "\n",
    "    user_ids, dynamic_xs, gest_ages, static_xs, ngo_hosp_ids, labels = features_dataset\n",
    "    \n",
    "    train_ids = train_beneficiaries['user_id']\n",
    "    idxes = [np.where(user_ids == x)[0][0] for x in train_ids]\n",
    "    train_static_features = static_xs[idxes]\n",
    "    train_static_features = train_static_features[:, : -8]\n",
    "\n",
    "    # test_ids = test_beneficiaries['user_id']\n",
    "    # idxes = [np.where(user_ids == x)[0][0] for x in test_ids]\n",
    "    # test_static_features = static_xs[idxes]\n",
    "    # test_static_features = test_static_features[:, : -8]\n",
    "    all_transition_probabilities = get_all_transition_probabilities(train_beneficiaries, train_transitions)\n",
    "    pass_to_kmeans_cols = ['P(L, N, L)', 'P(H, N, L)']\n",
    "\n",
    "    train_labels, centroids, _, cls, num_clusters, max_iters = kmeans_missing(all_transition_probabilities[pass_to_kmeans_cols], n_clusters, max_iter=100)\n",
    "    \n",
    "    # ipdb.set_trace()\n",
    "    train_beneficiaries['cluster'] = train_labels\n",
    "    # test_beneficiaries['cluster'] = cls.predict(test_static_features)\n",
    "    return train_static_features, train_labels\n",
    "    dt_clf = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", max_depth=30, n_jobs=-1, random_state=124)\n",
    "    dt_clf.fit(train_static_features, train_labels)\n",
    "\n",
    "    cluster_transition_probabilities = pd.DataFrame(columns=['cluster', 'count'] + cols)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        cluster_beneficiaries = train_beneficiaries[train_beneficiaries['cluster'] == i]\n",
    "        cluster_b_user_ids = cluster_beneficiaries['user_id']\n",
    "        probs, _ = get_transition_probabilities(cluster_b_user_ids, train_transitions, min_support=3)\n",
    "        probs['cluster'] = i\n",
    "        probs['count'] = len(cluster_b_user_ids)\n",
    "        cluster_transition_probabilities = cluster_transition_probabilities.append(probs, ignore_index=True)\n",
    "\n",
    "    # ipdb.set_trace()\n",
    "\n",
    "    return cluster_transition_probabilities, dt_clf\n",
    "\n",
    "def get_transition_probabilities(beneficiaries, transitions, min_support=3):\n",
    "    transitions = transitions[transitions['user_id'].isin(beneficiaries)]\n",
    "\n",
    "    i_transitions = transitions[transitions['action']=='Intervention']\n",
    "    n_i_transitions = transitions[transitions['action']=='No Intervention']\n",
    "\n",
    "    i_L = i_transitions[i_transitions['pre-action state']==\"L\"]\n",
    "    i_H = i_transitions[i_transitions['pre-action state']==\"H\"]\n",
    "\n",
    "    i_L_L = i_L[i_L['post-action state']==\"L\"]\n",
    "    i_L_H = i_L[i_L['post-action state']==\"H\"]\n",
    "\n",
    "    i_H_L = i_H[i_H['post-action state']==\"L\"]\n",
    "    i_H_H = i_H[i_H['post-action state']==\"H\"]\n",
    "\n",
    "    n_i_L = n_i_transitions[n_i_transitions['pre-action state']==\"L\"]\n",
    "    n_i_H = n_i_transitions[n_i_transitions['pre-action state']==\"H\"]\n",
    "\n",
    "    n_i_L_L = n_i_L[n_i_L['post-action state']==\"L\"]\n",
    "    n_i_L_H = n_i_L[n_i_L['post-action state']==\"H\"]\n",
    "\n",
    "    n_i_H_L = n_i_H[n_i_H['post-action state']==\"L\"]\n",
    "    n_i_H_H = n_i_H[n_i_H['post-action state']==\"H\"]\n",
    "\n",
    "    transition_probabilities = dict()\n",
    "    if i_L.shape[0] >= min_support:\n",
    "        transition_probabilities['P(L, I, L)'] = i_L_L.shape[0] / i_L.shape[0]\n",
    "        transition_probabilities['P(L, I, H)'] = i_L_H.shape[0] / i_L.shape[0]\n",
    "    else:\n",
    "        transition_probabilities['P(L, I, L)'] = np.nan\n",
    "        transition_probabilities['P(L, I, H)'] = np.nan\n",
    "\n",
    "    if i_H.shape[0] >= min_support:\n",
    "        transition_probabilities['P(H, I, L)'] = i_H_L.shape[0] / i_H.shape[0]\n",
    "        transition_probabilities['P(H, I, H)'] = i_H_H.shape[0] / i_H.shape[0]\n",
    "    else:\n",
    "        transition_probabilities['P(H, I, L)'] = np.nan\n",
    "        transition_probabilities['P(H, I, H)'] = np.nan\n",
    "    \n",
    "    if n_i_L.shape[0] >= min_support:\n",
    "        transition_probabilities['P(L, N, L)'] = n_i_L_L.shape[0] / n_i_L.shape[0]\n",
    "        transition_probabilities['P(L, N, H)'] = n_i_L_H.shape[0] / n_i_L.shape[0]\n",
    "    else:\n",
    "        transition_probabilities['P(L, N, L)'] = np.nan\n",
    "        transition_probabilities['P(L, N, H)'] = np.nan\n",
    "\n",
    "    if n_i_H.shape[0] >= min_support:\n",
    "        transition_probabilities['P(H, N, L)'] = n_i_H_L.shape[0] / n_i_H.shape[0]\n",
    "        transition_probabilities['P(H, N, H)'] = n_i_H_H.shape[0] / n_i_H.shape[0]\n",
    "    else:\n",
    "        transition_probabilities['P(H, N, L)'] = np.nan\n",
    "        transition_probabilities['P(H, N, H)'] = np.nan\n",
    "\n",
    "    return transition_probabilities, {'P(L, I, L)': i_L_L.shape[0], 'P(L, I, H)': i_L_H.shape[0], 'P(H, I, L)': i_H_L.shape[0], 'P(H, I, H)': i_H_H.shape[0], 'P(L, N, L)': n_i_L_L.shape[0], 'P(L, N, H)': n_i_L_H.shape[0], 'P(H, N, L)': n_i_H_L.shape[0], 'P(H, N, H)': n_i_H_H.shape[0]}\n",
    "\n",
    "def get_all_transition_probabilities(train_beneficiaries, transitions):\n",
    "    cols = [\n",
    "        \"P(L, I, L)\", \"P(L, I, H)\", \"P(H, I, L)\", \"P(H, I, H)\", \"P(L, N, L)\", \"P(L, N, H)\", \"P(H, N, L)\", \"P(H, N, H)\", \n",
    "    ]\n",
    "    transition_probabilities = pd.DataFrame(columns = ['user_id'] + cols)\n",
    "    user_ids = train_beneficiaries['user_id']\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        probs, _ = get_transition_probabilities([user_id], transitions, min_support=1)\n",
    "        probs['user_id'] = user_id\n",
    "\n",
    "        transition_probabilities = transition_probabilities.append(probs, ignore_index=True)\n",
    "\n",
    "    return transition_probabilities\n",
    "\n",
    "def get_group_transition_probabilities(train_beneficiaries, transitions, beneficiary_data):\n",
    "    features = [\"education\", \"phone_owner\", \"income_bracket\", \"age\"]\n",
    "    cols = [\n",
    "        \"P(L, I, L)\", \"P(L, I, H)\", \"P(H, I, L)\", \"P(H, I, H)\", \"P(L, N, L)\", \"P(L, N, H)\", \"P(H, N, L)\", \"P(H, N, H)\", \n",
    "    ]\n",
    "    education_s = [[7], [1], [2], [3], [4], [5], [6]]\n",
    "    phone_owner_s = [\"husband\", \"woman\", \"family\"]\n",
    "    income_bracket_s = [['5000-10000', '0-5000'], ['10000-15000', '15000-20000'], ['30000 and above'], ['25000-30000', '20000-25000']]\n",
    "    age_s = [(0, 19), (19, 24), (24, 29), (29, 36), (36, 100)]\n",
    "\n",
    "    transition_probabilities = pd.DataFrame(columns=features+cols)\n",
    "    for education_idx in range(len(education_s)):\n",
    "        for phone_owner_idx in range(len(phone_owner_s)):\n",
    "            for income_bracket_idx in range(len(income_bracket_s)):\n",
    "                for age_idx in range(len(age_s)):\n",
    "                    beneficiaries = beneficiary_data[\n",
    "                        (beneficiary_data['user_id'].isin(train_beneficiaries['user_id']))&\n",
    "                        (beneficiary_data['age']>=age_s[age_idx][0])&\n",
    "                        (beneficiary_data['age']<age_s[age_idx][1])&\n",
    "                        (beneficiary_data['education'].isin(education_s[education_idx]))&\n",
    "                        (beneficiary_data['phone_owner']==phone_owner_s[phone_owner_idx])&\n",
    "                        (beneficiary_data['income_bracket'].isin(income_bracket_s[income_bracket_idx]))\n",
    "                    ][\"user_id\"]\n",
    "                    probs, _ = get_transition_probabilities(beneficiaries, transitions)\n",
    "\n",
    "                    probs[\"education\"] = education_idx\n",
    "                    probs[\"income_bracket\"] = income_bracket_idx\n",
    "                    probs[\"phone_owner\"] = phone_owner_idx\n",
    "                    probs[\"age\"] = age_idx\n",
    "                    probs['count'] = beneficiaries.shape[0]\n",
    "\n",
    "                    transition_probabilities = transition_probabilities.append(probs, ignore_index=True)\n",
    "\n",
    "    return transition_probabilities\n",
    "\n",
    "def get_pooled_estimate(group_transition_probabilities, train_beneficiaries, transitions, beneficiary_data):\n",
    "    education_s = [[7], [1], [2], [3], [4], [5], [6]]\n",
    "    phone_owner_s = [\"husband\", \"woman\", \"family\"]\n",
    "    income_bracket_s = [['5000-10000', '0-5000'], ['10000-15000', '15000-20000'], ['30000 and above'], ['25000-30000', '20000-25000']]\n",
    "    age_s = [(0, 19), (19, 24), (24, 29), (29, 36), (36, 100)]\n",
    "\n",
    "    beneficiaries = []\n",
    "    for i in group_transition_probabilities.index:\n",
    "        education_idx = int(group_transition_probabilities.loc[i, \"education\"].item())\n",
    "        income_bracket_idx = int(group_transition_probabilities.loc[i, \"income_bracket\"].item())\n",
    "        phone_owner_idx = int(group_transition_probabilities.loc[i, \"phone_owner\"].item())\n",
    "        age_idx = int(group_transition_probabilities.loc[i, \"age\"].item())\n",
    "\n",
    "        group_beneficiaries = beneficiary_data[\n",
    "            (beneficiary_data['user_id'].isin(train_beneficiaries['user_id']))&\n",
    "            (beneficiary_data['age']>=age_s[age_idx][0])&\n",
    "            (beneficiary_data['age']<age_s[age_idx][1])&\n",
    "            (beneficiary_data['education'].isin(education_s[education_idx]))&\n",
    "            (beneficiary_data['phone_owner']==phone_owner_s[phone_owner_idx])&\n",
    "            (beneficiary_data['income_bracket'].isin(income_bracket_s[income_bracket_idx]))\n",
    "        ][\"user_id\"]\n",
    "        beneficiaries.append(group_beneficiaries)\n",
    "\n",
    "    beneficiaries = pd.concat(beneficiaries, axis=0)\n",
    "    probs, num_lst = get_transition_probabilities(beneficiaries, transitions)\n",
    "\n",
    "    return probs, num_lst\n",
    "\n",
    "def get_reward(state, action, m):\n",
    "    if state[0] == \"L\":\n",
    "        reward = 1.0\n",
    "    else:\n",
    "        reward = -1.0\n",
    "    if action == 'N':\n",
    "        reward += m\n",
    "\n",
    "    return reward\n",
    "\n",
    "def plan(transition_probabilities, CONFIG):\n",
    "\n",
    "    v_values = np.zeros((CONFIG['clusters'], len(CONFIG['problem']['states'])))\n",
    "    q_values = np.zeros((CONFIG['clusters'], len(CONFIG['problem']['states']), len(CONFIG['problem']['actions'])))\n",
    "    high_m_values = 1 * np.ones((CONFIG['clusters'], len(CONFIG['problem']['states'])))\n",
    "    low_m_values = -1 * np.ones((CONFIG['clusters'], len(CONFIG['problem']['states'])))\n",
    "\n",
    "    for cluster in range(CONFIG['clusters']):\n",
    "        print('Planning for Cluster {}'.format(cluster))\n",
    "        t_probs = np.zeros((len(CONFIG['problem']['states']), len(CONFIG['problem']['states']), len(CONFIG['problem']['actions'])))\n",
    "        two_state_probs = np.zeros((2, 2, 2))\n",
    "        for i in range(two_state_probs.shape[0]):\n",
    "            for j in range(two_state_probs.shape[1]):\n",
    "                for k in range(two_state_probs.shape[2]):\n",
    "                    s = CONFIG['problem']['orig_states'][i]\n",
    "                    s_prime = CONFIG['problem']['orig_states'][j]\n",
    "                    a = CONFIG['problem']['actions'][k]\n",
    "                    two_state_probs[i, j, k] = transition_probabilities.loc[transition_probabilities['cluster']==cluster, \"P(\" + s + \", \" + a + \", \" + s_prime + \")\"]\n",
    "        \n",
    "        t_probs[0 : 2, 2 : 4, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[2 : 4, 4 : 6, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[4 : 6, 6 : 8, 0] = two_state_probs[:, :, 0]\n",
    "        t_probs[6 : 8, 6 : 8, 0] = two_state_probs[:, :, 0]\n",
    "\n",
    "        t_probs[0 : 2, 2 : 4, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[2 : 4, 4 : 6, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[4 : 6, 6 : 8, 1] = two_state_probs[:, :, 0]\n",
    "        t_probs[6 : 8, 0 : 2, 1] = two_state_probs[:, :, 1]\n",
    "\n",
    "        max_q_diff = np.inf\n",
    "        prev_m_values, m_values = None, None\n",
    "        while max_q_diff > 1e-5:\n",
    "            prev_m_values = m_values\n",
    "            m_values = (low_m_values + high_m_values) / 2\n",
    "            if type(prev_m_values) != type(None) and abs(prev_m_values - m_values).max() < 1e-20:\n",
    "                break\n",
    "            max_q_diff = 0\n",
    "            v_values[cluster, :] = np.zeros((len(CONFIG['problem']['states'])))\n",
    "            q_values[cluster, :, :] = np.zeros((len(CONFIG['problem']['states']), len(CONFIG['problem']['actions'])))\n",
    "            delta = np.inf\n",
    "            while delta > 0.0001:\n",
    "                delta = 0\n",
    "                for i in range(t_probs.shape[0]):\n",
    "                    v = v_values[cluster, i]\n",
    "                    v_a = np.zeros((t_probs.shape[2],))\n",
    "                    for k in range(v_a.shape[0]):\n",
    "                        for j in range(t_probs.shape[1]):\n",
    "                            v_a[k] += t_probs[i, j, k] * (get_reward(CONFIG['problem']['states'][i], CONFIG['problem']['actions'][k], m_values[cluster, i]) + CONFIG[\"gamma\"] * v_values[cluster, j])\n",
    "\n",
    "                    v_values[cluster, i] = np.max(v_a)\n",
    "                    delta = max([delta, abs(v_values[cluster, i] - v)])\n",
    "\n",
    "            state_idx = -1\n",
    "            for state in range(q_values.shape[1]):\n",
    "                for action in range(q_values.shape[2]):\n",
    "                    for next_state in range(q_values.shape[1]):\n",
    "                        q_values[cluster, state, action] += t_probs[state, next_state, action] * (get_reward(CONFIG['problem']['states'][state], CONFIG['problem']['actions'][action], m_values[cluster, state]) + CONFIG[\"gamma\"] * v_values[cluster, next_state])\n",
    "                # print(state, q_values[cluster, state, 0], q_values[cluster, state, 1])\n",
    "\n",
    "            for state in range(q_values.shape[1]):\n",
    "                if abs(q_values[cluster, state, 1] - q_values[cluster, state, 0]) > max_q_diff:\n",
    "                    state_idx = state\n",
    "                    max_q_diff = abs(q_values[cluster, state, 1] - q_values[cluster, state, 0])\n",
    "\n",
    "            # print(q_values)\n",
    "            # print(low_m_values, high_m_values)\n",
    "            if max_q_diff > 1e-5 and q_values[cluster, state_idx, 0] < q_values[cluster, state_idx, 1]:\n",
    "                low_m_values[cluster, state_idx] = m_values[cluster, state_idx]\n",
    "            elif max_q_diff > 1e-5 and q_values[cluster, state_idx, 0] > q_values[cluster, state_idx, 1]:\n",
    "                high_m_values[cluster, state_idx] = m_values[cluster, state_idx]\n",
    "\n",
    "            # print(low_m_values, high_m_values, state_idx)\n",
    "            # ipdb.set_trace()\n",
    "    \n",
    "    m_values = (low_m_values + high_m_values) / 2\n",
    "\n",
    "    return q_values, m_values\n",
    "\n",
    "def count_overlaps(test_beneficiaries):\n",
    "    call_beneficiaries = test_beneficiaries[test_beneficiaries['Group']==\"Google-AI-Calls\"]\n",
    "    call_succ_beneficiaries = test_beneficiaries[\n",
    "        (test_beneficiaries['Group']==\"Google-AI-Calls\")&\n",
    "        (test_beneficiaries['Intervention Status']==\"Successful\")\n",
    "    ]\n",
    "    control_beneficiaries = test_beneficiaries[test_beneficiaries['Group']==\"Google-AI-Control\"]\n",
    "\n",
    "    good_call_response = test_beneficiaries[\n",
    "        (test_beneficiaries['user_id'].isin(call_beneficiaries['user_id']))&\n",
    "        (test_beneficiaries['Post-intervention Day: E2C Ratio']>=0.5)\n",
    "    ]['user_id']\n",
    "\n",
    "    good_succ_call_response = test_beneficiaries[\n",
    "        (test_beneficiaries['user_id'].isin(call_succ_beneficiaries['user_id']))&\n",
    "        (test_beneficiaries['Post-intervention Day: E2C Ratio']>=0.5)\n",
    "    ]['user_id']\n",
    "\n",
    "    good_control_response = test_beneficiaries[\n",
    "        (test_beneficiaries['user_id'].isin(control_beneficiaries['user_id']))&\n",
    "        (test_beneficiaries['Post-intervention Day: E2C Ratio']>=0.5)\n",
    "    ]['user_id']\n",
    "\n",
    "    print([call_beneficiaries.shape[0], call_succ_beneficiaries.shape[0], control_beneficiaries.shape[0]])\n",
    "\n",
    "    notes = np.array([\n",
    "        [good_call_response.shape[0], \n",
    "        good_succ_call_response.shape[0],\n",
    "        good_control_response.shape[0]]\n",
    "    ])\n",
    "\n",
    "    results = []\n",
    "    for k in [100, 200]:\n",
    "        top_k_call_beneficiaries = call_beneficiaries.iloc[:k, :]\n",
    "        top_k_good_call_beneficiaries = top_k_call_beneficiaries[top_k_call_beneficiaries['user_id'].isin(good_call_response)]\n",
    "\n",
    "        top_k_succ_call_beneficiaries = call_succ_beneficiaries.iloc[:k, :]\n",
    "        top_k_good_succ_call_beneficiaries = top_k_succ_call_beneficiaries[top_k_succ_call_beneficiaries['user_id'].isin(good_succ_call_response)]\n",
    "\n",
    "        top_k_control_beneficiaries = control_beneficiaries.iloc[:k, :]\n",
    "        top_k_good_control_beneficiaries = top_k_control_beneficiaries[top_k_control_beneficiaries['user_id'].isin(good_control_response)]\n",
    "\n",
    "        results.append([\n",
    "            top_k_good_call_beneficiaries.shape[0]/k, \n",
    "            top_k_good_succ_call_beneficiaries.shape[0]/k,\n",
    "            top_k_good_control_beneficiaries.shape[0]/k\n",
    "        ])\n",
    "\n",
    "    return np.array(results), notes\n",
    "\n",
    "def run_experiment(train_beneficiaries, train_transitions, test_beneficiaries, call_data, CONFIG, features_dataset):\n",
    "    cluster_transition_probabilities, test_beneficiaries = get_static_feature_clusters(train_beneficiaries, train_transitions, test_beneficiaries, features_dataset, CONFIG['clusters'])\n",
    "    cluster_transition_probabilities.to_csv('outputs/weekly_cluster_transition_probabilities_{}.csv'.format(CONFIG['clusters']))\n",
    "    q_values, m_values = plan(cluster_transition_probabilities, CONFIG)\n",
    "\n",
    "    print('-'*60)\n",
    "    print('M Values: {}'.format(m_values))\n",
    "    print('-'*60)\n",
    "\n",
    "    print('-'*60)\n",
    "    print('Q Values: {}'.format(q_values))\n",
    "    print('-'*60)\n",
    "\n",
    "    for i in tqdm(test_beneficiaries.index):\n",
    "        user_id = test_beneficiaries.loc[i, \"user_id\"]\n",
    "        state = int(test_beneficiaries.loc[i, \"state\"])\n",
    "        cluster = int(test_beneficiaries.loc[i, 'cluster'])\n",
    "\n",
    "        # test_beneficiaries.loc[i, \"Whittle Index\"] = q_values[cluster, state, 1] - q_values[cluster, state, 0]\n",
    "        test_beneficiaries.loc[i, \"Whittle Index\"] = m_values[cluster, state]\n",
    "\n",
    "    test_beneficiaries = test_beneficiaries.sort_values(by=\"Whittle Index\", ascending=False)\n",
    "    test_beneficiaries.to_csv('outputs/weekly_test_set_{}.csv'.format(CONFIG['clusters']))\n",
    "    results, notes = count_overlaps(test_beneficiaries)\n",
    "\n",
    "    return results, notes\n",
    "\n",
    "# results, avgs = run_and_repeat(beneficiary_splits, transitions, call_data, CONFIG, features_dataset)\n",
    "# print(\"-\"*60)\n",
    "# print(results)\n",
    "# print(avgs)\n",
    "# print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad73a941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 11:16:18,847 - INFO - load_data - Loading data from folder 'feb16-mar15_data'\n",
      "2021-10-07 11:16:18,860 - INFO - load_call_data - Found 1 files in 'feb16-mar15_data/call'.\n",
      "2021-10-07 11:16:18,861 - INFO - load_call_data - Loading and cleaning call data.\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.39s/it]\n",
      "2021-10-07 11:16:34,753 - WARNING - gest_age_to_list - introivr is not a valid gestation age.\n",
      "2021-10-07 11:16:34,774 - WARNING - gest_age_to_list - introivr is not a valid gestation age.\n",
      "2021-10-07 11:16:34,778 - WARNING - gest_age_to_list - introivr is not a valid gestation age.\n",
      "2021-10-07 11:16:34,782 - WARNING - gest_age_to_list - introivr is not a valid gestation age.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcf3c3f3a274f988679a7fb78c6d4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=136.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 11:16:35,018 - WARNING - gest_age_to_list - introivr is not a valid gestation age.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 11:16:35,350 - INFO - load_call_data - Call data contains 842588 relevant call records for 23516 beneficiaries.\n",
      "2021-10-07 11:16:35,369 - INFO - load_data - Successfully loaded and cleaned beneficiary and call data.\n",
      "2021-10-07 11:16:35,370 - INFO - load_data - Beneficiary data contains data for 23516 beneficiaries\n",
      "2021-10-07 11:16:35,379 - INFO - load_data - Call data contains 842588 call records for 23516 beneficiaries\n",
      "2021-10-07 11:16:35,379 - INFO - preprocess_and_make_dataset - Preprocessing beneficiary data.\n",
      "2021-10-07 11:16:36,387 - INFO - preprocess_and_make_dataset - Preprocessing beneficiary data completed.\n",
      "2021-10-07 11:16:36,388 - INFO - preprocess_and_make_dataset - Preprocessing call data.\n",
      "2021-10-07 11:16:39,410 - INFO - preprocess_and_make_dataset - Preprocessing call data completed.\n",
      "2021-10-07 11:16:39,411 - INFO - preprocess_and_make_dataset - Building the dataset.\n",
      "2021-10-07 11:16:39,471 - INFO - preprocess_and_make_dataset - Preproccessed data has 23386 beneficiaries and 836509 calls.\n",
      "2021-10-07 11:16:39,472 - INFO - _build_dataset - 23386 beneficiaries in beneficiary data.\n",
      "2021-10-07 11:16:39,579 - INFO - _build_dataset - 23386 beneficiaries have met 30-day program length requirement.\n",
      "2021-10-07 11:17:50,734 - INFO - _build_dataset - 23386 beneficiaries have met minimum 1 calls requirement.\n",
      "2021-10-07 11:17:50,740 - INFO - preprocess_and_make_dataset - Dataset successfully build for 23019 beneficiaries.\n",
      "<ipython-input-5-bb6d7fc2ccd6>:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pilot_gold_e2c = pilot_labels[:, 3] / pilot_labels[:, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def run_third_pilot(all_beneficiaries, transitions, call_data, CONFIG, features_dataset, pilot_data, beneficiary_data):\n",
    "pilot_data = 'feb16-mar15_data'\n",
    "pilot_beneficiary_data, pilot_call_data = load_data(pilot_data)\n",
    "inf_dataset = preprocess_and_make_dataset(pilot_beneficiary_data, pilot_call_data)\n",
    "pilot_call_data = _preprocess_call_data(pilot_call_data)\n",
    "pilot_user_ids, pilot_dynamic_xs, pilot_gest_age, pilot_static_xs, pilot_hosp_id, pilot_labels = inf_dataset\n",
    "pilot_gold_e2c = pilot_labels[:, 3] / pilot_labels[:, 2]\n",
    "pilot_gold_e2c_processed = np.nan_to_num(pilot_gold_e2c)\n",
    "pilot_gold_labels = (pilot_gold_e2c_processed < 0.5) * 1.0\n",
    "pilot_gold_labels = pilot_gold_labels.astype(np.int)\n",
    "\n",
    "# ipdb.set_trace()\n",
    "\n",
    "enroll_gest_age_mean = np.mean(inf_dataset[3][:, 0])\n",
    "days_to_first_call_mean = np.mean(inf_dataset[3][:, 7])\n",
    "\n",
    "# dynamic features preprocessing\n",
    "pilot_dynamic_xs = pilot_dynamic_xs.astype(np.float32)\n",
    "pilot_dynamic_xs[:, :, 2] = pilot_dynamic_xs[:, :, 2] / 60\n",
    "pilot_dynamic_xs[:, :, 3] = pilot_dynamic_xs[:, :, 3] / 60\n",
    "pilot_dynamic_xs[:, :, 4] = pilot_dynamic_xs[:, :, 4] / 12\n",
    "\n",
    "# static features preprocessing\n",
    "pilot_static_xs = pilot_static_xs.astype(np.float32)\n",
    "pilot_static_xs[:, 0] = (pilot_static_xs[:, 0] - enroll_gest_age_mean)\n",
    "pilot_static_xs[:, 7] = (pilot_static_xs[:, 7] - days_to_first_call_mean)\n",
    "\n",
    "dependencies = {\n",
    "    'BinaryAccuracy': BinaryAccuracy,\n",
    "    'F1': F1,\n",
    "    'Precision': Precision,\n",
    "    'Recall': Recall\n",
    "}\n",
    "\n",
    "#     model = load_model(os.path.join(\"models\", 'lstm_model_final', \"model\"), custom_objects=dependencies)\n",
    "#     output_probs = model.predict(x=[pilot_static_xs, pilot_dynamic_xs, pilot_hosp_id, pilot_gest_age])\n",
    "#     out_lstm_labels = (output_probs >= 0.5).astype(np.int)\n",
    "#     low_eng_idxes = np.where(out_lstm_labels == 1)[0]\n",
    "#     low_eng_user_ids = np.array(pilot_user_ids)[low_eng_idxes]\n",
    "pilot_static_features = np.array(pilot_static_xs, dtype=np.float)\n",
    "pilot_static_features = pilot_static_features[:, : -8]\n",
    "\n",
    "\n",
    "\n",
    "# run_third_pilot(all_beneficiaries, transitions, call_data, CONFIG, features_dataset, 'feb16-mar15_data', beneficiary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2afa5cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-3d9c7204032f>:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_beneficiaries['cluster'] = train_labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "cluster_transition_probabilities, cls = get_individual_transition_clusters(all_beneficiaries, transitions, features_dataset, CONFIG['clusters'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35756117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-388638399928>:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_beneficiaries['cluster'] = train_labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=30, n_estimators=200,\n",
       "                       n_jobs=-1, random_state=124)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_static_features, train_labels = get_individual_transition_clusters(all_beneficiaries, transitions, features_dataset, CONFIG['clusters'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2a6bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4238, 44), (4238,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_static_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5419ef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc:  0.04481132075471698 train acc:  0.9678466076696165\n",
      "test acc:  0.03773584905660377 train acc:  0.963716814159292\n",
      "test acc:  0.04834905660377359 train acc:  0.963716814159292\n",
      "test acc:  0.05306603773584906 train acc:  0.9648967551622419\n",
      "test acc:  0.03537735849056604 train acc:  0.9684365781710914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(train_static_features, train_labels):\n",
    "#     dt_clf = RandomForestClassifier(n_estimators=20, criterion=\"entropy\", max_depth=3, n_jobs=-1, random_state=124)\n",
    "    dt_clf = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", max_depth=30, n_jobs=-1, random_state=124)\n",
    "    dt_clf.fit(train_static_features[train_index], train_labels[train_index])\n",
    "    print('test acc: ', dt_clf.score(train_static_features[test_index], train_labels[test_index]), \\\n",
    "    'train acc: ', dt_clf.score(train_static_features[train_index], train_labels[train_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e73f49c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be2852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", max_depth=30, n_jobs=-1, random_state=124)\n",
    "dt_clf.fit(train_static_features, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
