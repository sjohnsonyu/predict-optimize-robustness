{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e60692-7dce-43e5-9ce0-ae28fa923f93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a45062a4-e8e1-4794-98cc-86fa6a28dca0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27419346, 0.670281  , 0.14842833],\n",
       "       [0.69723742, 0.48672025, 0.53546215],\n",
       "       [0.04288428, 0.92150141, 0.9273366 ],\n",
       "       [0.9269079 , 0.29699135, 0.99797212],\n",
       "       [0.47595611, 0.04947948, 0.1734023 ],\n",
       "       [0.2636093 , 0.5115926 , 0.57478398],\n",
       "       [0.34739073, 0.18492853, 0.07481354],\n",
       "       [0.83703536, 0.37887813, 0.37513082],\n",
       "       [0.32459255, 0.11141303, 0.94032076],\n",
       "       [0.08972007, 0.16788075, 0.6307363 ],\n",
       "       [0.0585818 , 0.55643061, 0.13133445],\n",
       "       [0.26573117, 0.5415581 , 0.56359827]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.random.rand(12,3)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f68eb6-72bf-4e34-b78b-52450b135c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Create a restless MAB with N arms\n",
    "\n",
    "Step 1: Cluster all the beneficiaries using the PPF strategy - where we cluster based on the transition probabilities\n",
    "\n",
    "Step 2: Model transition probabilities for each cluster\n",
    "\n",
    "Do this in a way that balances exploration and exploitation via Thompson sampling\n",
    "\n",
    "Start with Prior beliefs for each arm \n",
    "Start with the same belief - Flat/uninformative prior\n",
    "\n",
    "Where each arm is an MDP (MDP cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f27cb8e-d4a5-407b-a788-57821af1b720",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace\n",
    "\n",
    "stationary=True\n",
    "class Bandit():\n",
    "  def __init__(self, arm_count):\n",
    "    \"\"\"\n",
    "    Multi-armed bandit with rewards 1 or 0.\n",
    "    \n",
    "    At initialization, multiple arms are created. The probability of each arm\n",
    "    returning reward 1 if pulled is sampled from Bernouilli(p), where p randomly\n",
    "    chosen from Uniform(0,1) at initialization\n",
    "    \"\"\"\n",
    "    self.arm_count = arm_count\n",
    "    self.generate_thetas()\n",
    "    self.timestep = 0\n",
    "    global stationary\n",
    "    self.stationary=stationary\n",
    "    \n",
    "  def generate_thetas(self):\n",
    "    self.thetas = np.random.uniform(0,1,self.arm_count)\n",
    "  \n",
    "  def get_reward_regret(self, arm):\n",
    "    \"\"\" Returns random reward for arm action. Assumes actions are 0-indexed\n",
    "    Args:\n",
    "      arm is an int\n",
    "    \"\"\"\n",
    "    self.timestep += 1\n",
    "    if (self.stationary==False) and (self.timestep%100 == 0) :\n",
    "      self.generate_thetas()\n",
    "    # Simulate bernouilli sampling\n",
    "    sim = np.random.uniform(0,1,self.arm_count)\n",
    "    rewards = (sim<self.thetas).astype(int)\n",
    "    reward = rewards[arm]\n",
    "    regret = self.thetas.max() - self.thetas[arm]\n",
    "    \n",
    "    return reward, regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1def5ad-6bad-47ce-9bbb-c2a3d49f4896",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BetaAlgo():\n",
    "  \"\"\"\n",
    "  The algos try to learn which Bandit arm is the best to maximize reward.\n",
    "  \n",
    "  It does this by modelling the distribution of the Bandit arms with a Beta, \n",
    "  assuming the true probability of success of an arm is Bernouilli distributed.\n",
    "  \"\"\"\n",
    "  def __init__(self, bandit):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      bandit: the bandit class the algo is trying to model\n",
    "    \"\"\"\n",
    "    self.bandit = bandit\n",
    "    self.arm_count = bandit.arm_count\n",
    "    self.alpha = np.ones(self.arm_count)\n",
    "    self.beta = np.ones(self.arm_count)\n",
    "  \n",
    "  def get_reward_regret(self, arm):\n",
    "    reward, regret = self.bandit.get_reward_regret(arm)\n",
    "    self._update_params(arm, reward)\n",
    "    return reward, regret\n",
    "  \n",
    "  def _update_params(self, arm, reward):\n",
    "    self.alpha[arm] += reward\n",
    "    self.beta[arm] += 1 - reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcfdc32c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class BernGreedy(BetaAlgo):\n",
    "  def __init__(self, bandit):\n",
    "    super().__init__(bandit)\n",
    "  \n",
    "  @staticmethod\n",
    "  def name():\n",
    "    return 'beta-greedy'\n",
    "   \n",
    "  def get_action(self):\n",
    "    \"\"\" Bernouilli parameters are the expected values of the beta\"\"\"\n",
    "    theta = self.alpha / (self.alpha + self.beta)\n",
    "    return theta.argmax()\n",
    "  \n",
    "class BernThompson(BetaAlgo):\n",
    "  def __init__(self, bandit):\n",
    "    super().__init__(bandit)\n",
    "\n",
    "  @staticmethod\n",
    "  def name():\n",
    "    return 'thompson'\n",
    "  \n",
    "  def get_action(self):\n",
    "    \"\"\" Bernouilli parameters are sampled from the beta\"\"\"\n",
    "    theta = np.random.beta(self.alpha, self.beta)\n",
    "    return theta.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d658a4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "class EpsilonGreedy():\n",
    "  \"\"\"\n",
    "  Epsilon Greedy with incremental update.\n",
    "  Based on Sutton and Barto pseudo-code, page. 24\n",
    "  \"\"\"\n",
    "  def __init__(self, bandit):\n",
    "    global epsilon\n",
    "    self.epsilon = epsilon\n",
    "    self.bandit = bandit\n",
    "    self.arm_count = bandit.arm_count\n",
    "    self.Q = np.zeros(self.arm_count) # q-value of actions\n",
    "    self.N = np.zeros(self.arm_count) # action count\n",
    "  \n",
    "  @staticmethod\n",
    "  def name():\n",
    "    return 'epsilon-greedy'\n",
    "  \n",
    "  def get_action(self):\n",
    "    if np.random.uniform(0,1) > self.epsilon:\n",
    "      action = self.Q.argmax()\n",
    "    else:\n",
    "      action = np.random.randint(0, self.arm_count)\n",
    "    return action\n",
    "  \n",
    "  def get_reward_regret(self, arm):\n",
    "    reward, regret = self.bandit.get_reward_regret(arm)\n",
    "    self._update_params(arm, reward)\n",
    "    return reward, regret\n",
    "  \n",
    "  def _update_params(self, arm, reward):\n",
    "    self.N[arm] += 1 # increment action count\n",
    "    self.Q[arm] += 1/self.N[arm] * (reward - self.Q[arm]) # inc. update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964d718",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# lorem ipsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "875ce99b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ucb_c = 2\n",
    "class UCB():\n",
    "  \"\"\"\n",
    "  Epsilon Greedy with incremental update.\n",
    "  Based on Sutton and Barto pseudo-code, page. 24\n",
    "  \"\"\"\n",
    "  def __init__(self, bandit):\n",
    "    global ucb_c\n",
    "    self.ucb_c = ucb_c\n",
    "    self.bandit = bandit\n",
    "    self.arm_count = bandit.arm_count\n",
    "    self.Q = np.zeros(self.arm_count) # q-value of actions\n",
    "    self.N = np.zeros(self.arm_count) + 0.0001 # action count\n",
    "    self.timestep = 1\n",
    "  \n",
    "  @staticmethod\n",
    "  def name():\n",
    "    return 'ucb'\n",
    "  \n",
    "  def get_action(self):\n",
    "    ln_timestep = np.log(np.full(self.arm_count, self.timestep))\n",
    "    confidence = self.ucb_c * np.sqrt(ln_timestep/self.N)\n",
    "    action = np.argmax(self.Q + confidence)\n",
    "    self.timestep += 1\n",
    "    return action\n",
    "  \n",
    "  def get_reward_regret(self, arm):\n",
    "    reward, regret = self.bandit.get_reward_regret(arm)\n",
    "    self._update_params(arm, reward)\n",
    "    return reward, regret\n",
    "  \n",
    "  def _update_params(self, arm, reward):\n",
    "    self.N[arm] += 1 # increment action count\n",
    "    self.Q[arm] += 1/self.N[arm] * (reward - self.Q[arm]) # inc. update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4327c15",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_data(y):\n",
    "  \"\"\" y is a 1D vector \"\"\"\n",
    "  x = np.arange(y.size)\n",
    "  _ = plt.plot(x, y, 'o')\n",
    "  \n",
    "def multi_plot_data(data, names):\n",
    "  \"\"\" data, names are lists of vectors \"\"\"\n",
    "  x = np.arange(data[0].size)\n",
    "  for i, y in enumerate(data):\n",
    "    plt.plot(x, y, 'o', markersize=2, label=names[i])\n",
    "  plt.legend(loc='upper right', prop={'size': 16}, numpoints=10)\n",
    "  plt.show()\n",
    "  \n",
    "def simulate(simulations, timesteps, arm_count, Algorithm):\n",
    "  \"\"\" Simulates the algorithm over 'simulations' epochs \"\"\"\n",
    "  sum_regrets = np.zeros(timesteps)\n",
    "  for e in range(simulations):\n",
    "    bandit = Bandit(arm_count)\n",
    "    algo = Algorithm(bandit)\n",
    "    regrets = np.zeros(timesteps)\n",
    "    for i in range(timesteps):\n",
    "      action = algo.get_action()\n",
    "      reward, regret = algo.get_reward_regret(action)\n",
    "      regrets[i] = regret\n",
    "    sum_regrets += regrets  \n",
    "  mean_regrets = sum_regrets / simulations\n",
    "  return mean_regrets\n",
    "\n",
    "def experiment(arm_count, timesteps=1000, simulations=1000):\n",
    "  \"\"\" \n",
    "  Standard setup across all experiments \n",
    "  Args:\n",
    "    timesteps: (int) how many steps for the algo to learn the bandit\n",
    "    simulations: (int) number of epochs\n",
    "  \"\"\"\n",
    "  algos = [EpsilonGreedy, UCB, BernThompson]\n",
    "  regrets = []\n",
    "  names = []\n",
    "  for algo in algos:\n",
    "    regrets.append(simulate(simulations, timesteps, arm_count, algo))\n",
    "    names.append(algo.name())\n",
    "  multi_plot_data(regrets, names)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
